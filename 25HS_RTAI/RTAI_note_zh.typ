#import "../assets/tmp_nt.typ": *

#show: summary_project.with(
  title: "25HS_RTAI_Note",
  authors: ((name: ""),),
  base_size: 9pt,
  heading1_size: 1.3em,
  heading2_size: 1.2em,
  math_size: 0.95em,
  par_spacing: 0.5em,
  par_leading: 0.5em,
  primary_color: rgb("#997933"),
  secondary_color: rgb("#2E7D5A"),
  margin: (x: 1.25cm, y: 1.25cm),
)

#pagebreak()

= Part 1: Introduction <sec:intro>

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    == è¯¾ç¨‹ Verticals

    è¯¾ç¨‹å›´ç»•ä¸‰ä¸ªç ”ç©¶æ–¹å‘å±•å¼€ï¼š

    #figure(
      table(
        columns: 3,
        align: center,
        fill: (x, y) => if y == 0 { c1 },
        [*Attacks & Guarantees*], [*Privacy*], [*Provenance*],
        [Convex Relaxation], [Membership Inference], [Watermarking],
        [Certified Training], [Differential Privacy], [Benchmark Eval],
        [Randomized Smoothing], [Federated Learning], [Contamination],
      ),
      caption: [RTAI ä¸‰å¤§æ–¹å‘],
    )
  ],
  [
    == æ ¸å¿ƒé—®é¢˜

    æ‰€æœ‰æ–¹æ³•éƒ½å›´ç»•ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜å±•å¼€ï¼š

    $ forall delta in cal(B)(x): quad f(x + delta) = f(x) ? $

    å…¶ä¸­ $cal(B)(x) = {x' mid(|) norm(x' - x)_p lt.eq epsilon}$ æ˜¯æ‰°åŠ¨é›†åˆã€‚

    #note[
      å…³é”®å¼ åŠ›åœ¨äº *å­˜åœ¨é‡è¯ $exists$* vs *å…¨ç§°é‡è¯ $forall$*ï¼š
      - Attackï¼šè¯æ˜ $exists delta$ï¼ˆæ‰¾åä¾‹ï¼‰
      - Verificationï¼šè¯æ˜ $forall delta$ï¼ˆæ€§è´¨æˆç«‹ï¼‰
      - Defenseï¼šæ„é€  $f_theta$ ä½¿ Verification æˆåŠŸ
    ]
  ],
)

== Robustness Verification

=== é—®é¢˜å®šä¹‰

#definition(title: "Robustness Verification")[
  ç»™å®šç½‘ç»œ $f$ å’Œè¾“å…¥è§„æ ¼#footnote[Input Specificationï¼Œå®šä¹‰äº†å…è®¸çš„æ‰°åŠ¨èŒƒå›´] $Phi(x)$ï¼ŒéªŒè¯ï¼š
  $ forall x' in Phi(x): f(x') = f(x) $

  å…¶ä¸­ $Phi(x)$ é€šå¸¸æ˜¯ $ell_p$ çƒï¼š$Phi(x) = {x' mid(|) norm(x' - x)_p lt.eq epsilon}$
]

#grid(
  columns: (1.5fr, 1fr),
  gutter: 1em,
  [
    ä¸ºä»€ä¹ˆéš¾ï¼Ÿè€ƒè™‘ MNISTï¼š
    - è¾“å…¥dimï¼š784
    - å¯èƒ½çš„æ‰°åŠ¨ï¼š$2^(784)$ ç§ï¼ˆç©·ä¸¾ä¸å¯è¡Œï¼‰
    - å³ä½¿æ˜¯ $ell_infinity$ çƒï¼Œå†…éƒ¨ç‚¹æ•°ä¹Ÿè¶‹è¿‘æ— ç©·

    è§£å†³æ€è·¯ï¼šä¸æšä¸¾æ¯ä¸ªç‚¹ï¼Œè€Œæ˜¯*æŠŠæ•´ä¸ªå‡¸å½¢çŠ¶æ¨è¿‡ç½‘ç»œ*ã€‚
  ],
  [
    ```
    Input Space    â†’    Output Space
    â”Œâ”€â”€â”€â”€â”€â”€â”
    â”‚ â—â—â—â— â”‚  â”€â”€fâ”€â”€â†’   Decision Region
    â”‚ â—â—â—â— â”‚
    â””â”€â”€â”€â”€â”€â”€â”˜
      Îµ-ball           å…¨åœ¨åŒä¸€ç±»ï¼Ÿ
    ```
  ],
)

=== Certification æ–¹æ³•å¯¹æ¯”

ä¸¤ç±»ä¸»è¦æ–¹æ³•æä¾›ä¸åŒç±»å‹çš„ä¿è¯ï¼š

#figure(
  table(
    columns: 3,
    align: left,
    [], [*Convex Methods*], [*Randomized Smoothing*],
    [åŸç†], [ä¼ æ’­å‡¸é›†åˆé€šè¿‡ç½‘ç»œ], [é‡‡æ · + ç»Ÿè®¡ä¿è¯],
    [æ˜¯å¦éœ€è¦ç‰¹æ®Šè®­ç»ƒ], [éœ€è¦ Certified Training], [æ¨ç†æ—¶å³å¯ä½¿ç”¨],
    [å¯éªŒè¯æ€§è´¨], [å¤šç§ï¼ˆrobustness, fairness ç­‰ï¼‰], [æœ‰é™],
    [å¯æ‰©å±•æ€§], [å°åˆ°ä¸­å‹ç½‘ç»œ], [å¤§modelï¼ˆåŒ…æ‹¬ LLMï¼‰],
    [ä¿è¯ç±»å‹], [ç¡®å®šæ€§], [probæ€§],
  ),
  caption: [ä¸¤ç±» Certification æ–¹æ³•å¯¹æ¯”],
)

#note[
  é€‰æ‹©å“ªç§æ–¹æ³•å–å†³äºåº”ç”¨åœºæ™¯ï¼š
  - éœ€è¦ç¡®å®šæ€§ä¿è¯ï¼šConvex Methods
  - éœ€è¦æ‰©å±•åˆ°å¤§modelï¼šRandomized Smoothing
  - éœ€è¦è®­ç»ƒæ—¶ä¼˜åŒ–ï¼šCertified Training
]

== Min-Max ä¼˜åŒ–æ¡†æ¶

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === Standard Training

    $ min_theta bb(E)_((x,y) tilde cal(D)) [cal(L)(f_theta (x), y)] $

    åªå…³å¿ƒå•ç‚¹ $x$ çš„å‡†ç¡®ç‡ã€‚
  ],
  [
    === Robust Training

    $ min_theta bb(E)_((x,y) tilde cal(D)) [max_(x' in Phi(x)) cal(L)(f_theta (x'), y)] $

    å…³å¿ƒæ•´ä¸ª $Phi(x)$ å†…çš„ worst-caseã€‚
  ],
)

#definition(title: "Min-Max åŒå±‚ä¼˜åŒ–")[
  Robust Training æ˜¯åµŒå¥—ä¼˜åŒ–é—®é¢˜ï¼š
  - *Inner max*ï¼šåœ¨ $Phi(x)$ å†…æ‰¾ worst-case æ‰°åŠ¨ï¼ˆæ”»å‡»è€…è§†è§’ï¼‰
  - *Outer min*ï¼šä¼˜åŒ–modelå‚æ•° $theta$ æŠµå¾¡æœ€åæƒ…å†µï¼ˆé˜²å¾¡è€…è§†è§’ï¼‰

  ä¸¤è€…å½¢æˆå¯¹æŠ—åšå¼ˆã€‚
]

è¿™ä¸ªæ¡†æ¶ç»Ÿä¸€äº†ä¸‰èŠ‚è¯¾çš„å†…å®¹ï¼š

#figure(
  table(
    columns: 4,
    align: left,
    [*ä»»åŠ¡*], [*Inner Max*], [*Outer Min*], [*æ–¹æ³•*],
    [Attack], [æ‰¾ worst-case $delta^*$], [â€”ï¼ˆ$theta$ å›ºå®šï¼‰], [FGSM/PGD/C&W],
    [Adversarial Training], [PGD ç”Ÿæˆå¯¹æŠ—sample], [SGD æ›´æ–°æƒé‡], [PGD-AT],
    [Certified Training], [Convex Relaxation æ¨åŒºåŸŸ], [ä¼˜åŒ– certified loss], [IBP/CROWN],
  ),
  caption: [Min-Max æ¡†æ¶çš„ä¸‰ç§å®ä¾‹åŒ–],
)

== Robustness â‰ˆ Individual Fairness

#theorem(title: "æŠ€æœ¯ç­‰ä»·æ€§")[
  Robustness å’Œ Individual Fairness åœ¨æ•°å­¦ä¸Šç­‰ä»·ï¼ŒåŒºåˆ«ä»…åœ¨è·ç¦»åº¦é‡ $d$ çš„å®šä¹‰ï¼š

  *Robustness*ï¼š$forall x': d(x, x') lt.eq epsilon arrow.r.double f(x') = f(x)$

  *Individual Fairness*ï¼š$forall x': d_("sensitive")(x, x') lt.eq epsilon arrow.r.double f(x') = f(x)$
]

#note[
  å®é™…æ„ä¹‰ï¼šåŒä¸€å¥— Convex Relaxation æŠ€æœ¯å¯ä»¥ç”¨äºï¼š
  - Robustnessï¼š$d$ æ˜¯åƒç´ çº§ $ell_p$ è·ç¦»
  - Fairnessï¼š$d$ åªçœ‹æ•æ„Ÿå±æ€§ï¼ˆå¦‚ç§æ—ã€æ€§åˆ«ï¼‰çš„å·®å¼‚
  - Quantizationï¼š$d$ æ˜¯é‡åŒ–è¯¯å·®èŒƒå›´
]

== Differential Privacy

#definition(title: "$(epsilon, delta)$-Differential Privacy")[
  ç®—æ³• $M$ æ»¡è¶³ $(epsilon, delta)$-DPï¼Œè‹¥å¯¹æ‰€æœ‰ç›¸å·®ä¸€æ¡è®°å½•çš„æ•°æ®åº“ $D, D'$ï¼š
  $ P[M(D) in S] lt.eq e^epsilon dot P[M(D') in S] + delta $

  ç›´è§‰ï¼šåŠ å…¥/ç§»é™¤ä¸€ä¸ªäººï¼Œè¾“å‡ºåˆ†å¸ƒå˜åŒ–å¾ˆå° â†’ æ— æ³•æ¨æ–­ä¸ªä½“æ˜¯å¦åœ¨æ•°æ®ä¸­ã€‚
]

#figure(
  table(
    columns: 3,
    align: left,
    [*ç¬¦å·*], [*å«ä¹‰*], [*å¤‡æ³¨*],
    [$M$], [éšæœºåŒ–ç®—æ³•], [å¦‚ DP-SGD è®­ç»ƒè¿‡ç¨‹],
    [$D, D'$], [ç›¸å·®ä¸€æ¡è®°å½•çš„æ•°æ®åº“], ["é‚»å±…"dataset],
    [$epsilon$], [éšç§é¢„ç®—], [è¶Šå°è¶Šéšç§],
    [$delta$], [å¤±è´¥prob], [é€šå¸¸å– $lt.double 1/n$],
  ),
  caption: [DP ç¬¦å·è¯´æ˜],
)



= Part 2: Verification <sec:verification>

== Verification é—®é¢˜å½¢å¼åŒ–

#definition(title: "Formal Verification Problem")[
  $ forall i in I: phi(i) arrow.r.double.long N(i) tack.double C $

  å¯¹æ‰€æœ‰è¾“å…¥ $i$ï¼Œè‹¥æ»¡è¶³å‰æ¡ä»¶ $phi(i)$ï¼Œåˆ™ç½‘ç»œè¾“å‡º $N(i)$ æ»¡è¶³åæ¡ä»¶ $C$ã€‚
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    #definition(title: "Sound")[
      è‹¥æ–¹æ³•è¯´"æˆç«‹"ï¼Œåˆ™æ€§è´¨*ç¡®å®æˆç«‹*ã€‚
      $ "Proved" arrow.r.double "True" $

      å®å¯å¤šè¯´"ä¸çŸ¥é“"ï¼ˆä¿å®ˆï¼‰ï¼Œä¹Ÿä¸èƒ½è¯¯æŠ¥"å®‰å…¨"ã€‚
    ]
  ],
  [
    #definition(title: "Complete")[
      è‹¥æ€§è´¨*ç¡®å®æˆç«‹*ï¼Œåˆ™æ–¹æ³•*èƒ½å¤Ÿè¯æ˜*ã€‚
      $ "True" arrow.r.double "Provable" $

      ä¸ä¼šé—æ¼å¯è¯æ˜çš„æ€§è´¨ã€‚
    ]
  ],
)

#tip[
  å¤§å¤šæ•°å®ç”¨æ–¹æ³•æ˜¯ *Sound but Incomplete*ï¼š
  - è¯´"å®‰å…¨"æ—¶å¯ä¿¡
  - è¯´"ä¸çŸ¥é“"æ—¶ä¸ä»£è¡¨ä¸å®‰å…¨ï¼Œå¯èƒ½åªæ˜¯æ–¹æ³•èƒ½åŠ›æœ‰é™
]

```
                    Property Actually Holds?
                         YES        NO
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    Method says     â”‚  âœ“ OK    â”‚ âœ— UNSOUNDâ”‚
       "HOLDS"      â”‚          â”‚  (å±é™©!) â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    Method says     â”‚INCOMPLETEâ”‚  âœ“ OK    â”‚
     "UNKNOWN"      â”‚  (ä¿å®ˆ)   â”‚          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

== Box Relaxation (IBP)

#definition(title: "Interval Bound Propagation")[
  ç”¨åŒºé—´ $[l, u]$ è¡¨ç¤ºç¥ç»å…ƒå¯èƒ½çš„å–å€¼èŒƒå›´ã€‚åŒºé—´å½¢æˆ hyper-rectangleï¼ˆBoxï¼‰ã€‚

  $ "Box": quad [l_1, u_1] times [l_2, u_2] times dots.c times [l_n, u_n] $
]

=== Abstract Transformers

å¯¹æ¯ç§è¿ç®—å®šä¹‰åœ¨åŒºé—´ä¸Šçš„æ“ä½œï¼š

#figure(
  table(
    columns: 2,
    align: left,
    [*æ“ä½œ*], [*ç¬¦å·å®šä¹‰*],
    [åŠ æ³•], [$[a,b] plus.o [c,d] = [a+c, b+d]$],
    [å–è´Ÿ], [$-[a,b] = [-b, -a]$],
    [æ ‡é‡ä¹˜], [$lambda [a,b] = cases([lambda a\, lambda b] & lambda gt.eq 0, [lambda b\, lambda a] & lambda < 0)$],
    [ReLU], [$"ReLU"([l, u]) = [max(0, l), max(0, u)]$],
  ),
  caption: [Box Abstract Transformers],
)

=== Affine å±‚çš„ä¼ æ’­

å¯¹äº $bold(z) = W bold(x) + bold(b)$ï¼Œç²¾ç¡®è®¡ç®—åŒºé—´ï¼š

$ [bold(l)', bold(u)'] = [W^+ bold(l) + W^- bold(u) + bold(b), quad W^+ bold(u) + W^- bold(l) + bold(b)] $

å…¶ä¸­ $W^+ = max(W, 0)$ï¼Œ$W^- = min(W, 0)$ã€‚

=== Crossing ReLU

#definition(title: "Crossing ReLU")[
  è‹¥ ReLU è¾“å…¥çš„ bounds æ»¡è¶³ $l < 0 < u$ï¼Œåˆ™ç§°è¯¥ ReLU å¤„äº *crossing* çŠ¶æ€ã€‚

  é crossing æƒ…å†µæ›´ç®€å•ï¼š
  - $l gt.eq 0$ï¼šæ’æ­£ï¼Œ$y = x$ï¼ˆç›´æ¥ä¼ é€’ï¼‰
  - $u lt.eq 0$ï¼šæ’è´Ÿï¼Œ$y = 0$ï¼ˆè¾“å‡ºæ’ä¸º 0ï¼‰
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    ```
    Crossing ReLU (l < 0 < u):
            Y
            â”‚      /
            â”‚     / â† çœŸå® ReLU
            â”‚    /
            â”‚   /â–ˆâ–ˆâ–ˆâ–ˆ â† Over-approx åŒºåŸŸ
            â”‚  /â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
            â”‚ /â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
       â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ X
          L â”‚        U
    ```
  ],
  [
    Crossing ReLU å¼•å…¥ *over-approximation*ï¼š
    - çœŸå®å¯è¾¾é›†ï¼ˆé»„è‰²ç¢ç‰‡ï¼‰
    - è¿‘ä¼¼åŒ…ç»œï¼ˆç´«è‰² Boxï¼‰
    - ç´«è‰² $supset.eq$ é»„è‰²ï¼ˆ*Sound*ï¼‰
    - é¢å¤–åŒºåŸŸç§°ä¸º "garbage points"
  ],
)

#tip[
  Over-approximation è¯¯å·®éšæ·±åº¦*ç´¯ç§¯*ï¼š
  $ "Error"_k = f("Error"_(k-1), "Layer"_k) $

  æ·±åº¦æ˜¯ bound propagation çš„æ•Œäººã€‚
]

== MILP ç¼–ç 

Mixed Integer Linear Programming é€šè¿‡å¼•å…¥äºŒè¿›åˆ¶å˜é‡å®ç° *Sound & Complete* éªŒè¯ã€‚

#definition(title: "Crossing ReLU çš„ MILP ç¼–ç ")[
  å¯¹äº crossing ReLUï¼ˆ$l < 0 < u$ï¼‰ï¼Œå¼•å…¥äºŒè¿›åˆ¶å˜é‡ $a in {0, 1}$ï¼š

  $ y gt.eq x $
  $ y lt.eq x - l(1-a) $
  $ y lt.eq u dot a $
  $ y gt.eq 0 $

  - å½“ $a = 1$ï¼šçº¦æŸç®€åŒ–ä¸º $y = x$ï¼ˆactive branchï¼‰
  - å½“ $a = 0$ï¼šçº¦æŸç®€åŒ–ä¸º $y = 0$ï¼ˆinactive branchï¼‰
]

#theorem(title: "MILP å¤æ‚åº¦")[
  å¤æ‚åº¦ä¸º $O(2^k)$ï¼Œå…¶ä¸­ $k$ æ˜¯ *Crossing ReLU æ•°é‡*ï¼Œè€Œéæ€»ç¥ç»å…ƒæ•°ã€‚

  $ "Complexity" prop 2^(|{"Crossing ReLUs"}|) $
]

#note[
  è¿™æ„å‘³ç€ï¼š
  - æ›´ tight çš„ bounds â†’ æ›´å°‘ crossing â†’ MILP æ›´å¿«
  - Box/DeepPoly é¢„è®¡ç®— bounds å¯ä»¥å¤§å¹…å‡å°‘ MILP åˆ†æ”¯æ•°
  - è®ºæ–‡å£°ç§°"éªŒè¯ç™¾ä¸‡ç¥ç»å…ƒ"æ—¶ï¼Œè¦æ£€æŸ¥ crossing æ•°é‡å’Œç½‘ç»œå‡†ç¡®ç‡
]

=== å…·ä½“ä¾‹å­

ç»™å®šï¼š$x_1 in [0, 0.5], quad x_2 in [0.2, 0.7]$

Affine å±‚ï¼š$x_3 = x_1 + x_2, quad x_4 = x_1 - x_2$

Box ä¼ æ’­ï¼š
$ x_3 in [0 + 0.2, 0.5 + 0.7] = [0.2, 1.2] quad "ï¼ˆé crossingï¼Œ" l gt.eq 0 "ï¼‰" $
$ x_4 in [0 - 0.7, 0.5 - 0.2] = [-0.7, 0.3] quad "ï¼ˆCrossingï¼" l < 0 < u "ï¼‰" $

ç»“è®ºï¼š$x_3$ æ— éœ€åˆ†æ”¯ï¼ˆ$y = x$ï¼‰ï¼Œ$x_4$ éœ€è¦ MILP äºŒè¿›åˆ¶å˜é‡ã€‚

== DeepPoly Relaxation

#grid(
  columns: (1fr,1fr),
  [
    DeepPoly æ˜¯ä»‹äº Box å’Œ MILP ä¹‹é—´çš„æ–¹æ³•ï¼šæ¯” Box ç²¾ç¡®ï¼Œæ¯” MILP å¿«ã€‚

#definition(title: "Linear Symbolic Bounds")[
  æ¯ä¸ªç¥ç»å…ƒ $X_j$ ç»´æŠ¤four *constraints*ï¼š

  $ X_j gt.eq sum_i a_i^L X_i + b^L quad "(ä¸‹ç•Œçº¿æ€§çº¦æŸ)" $
  $ X_j lt.eq sum_i a_i^U X_i + b^U quad "(ä¸Šç•Œçº¿æ€§çº¦æŸ)" $
  $ X_j gt.eq L_j quad "(å…·ä½“ä¸‹ç•Œ)" $
  $ X_j lt.eq U_j quad "(å…·ä½“ä¸Šç•Œ)" $
]
  #note[
  ä¸ºä»€ä¹ˆéœ€è¦å…·ä½“ bounds $L_j, U_j$ï¼Ÿ
  - åˆ¤æ–­ ReLU æ˜¯å¦ crossing
  - éšæ—¶åœæ­¢ back-substitutionï¼ˆä¸å¿…å›æº¯åˆ°è¾“å…¥å±‚ï¼‰
  - è®¡ç®—æ•ˆç‡ï¼š$O(1)$ çš„ ReLU transformer
]
  ],
  [
    === Affine å±‚

å¯¹äº $bold(z) = W bold(x) + bold(b)$ï¼ŒDeepPoly æ˜¯ *Exact*ï¼ˆæ— æŸï¼‰ï¼š
$ bold(z) lt.eq W bold(x) + bold(b) lt.eq bold(z) quad "(upper = lower)" $

=== ReLU å±‚

å¯¹äº crossing ReLU $Y = "ReLU"(X)$ï¼Œå…¶ä¸­ $X in [l, u]$ ä¸” $l < 0 < u$ï¼š

*Upper boundï¼ˆå›ºå®šï¼‰*ï¼š
$ Y lt.eq frac(u, u - l)(X - l) = lambda X - lambda l, quad "where" lambda = frac(u, u - l) $

*Lower boundï¼ˆå¯é€‰/å¯ä¼˜åŒ–ï¼‰*ï¼š
$ Y gt.eq alpha X, quad "where" alpha in [0, 1] $

#tip[
  è¿™ä¸ª $alpha$ å°±æ˜¯ $alpha$-CROWN ä¸­çš„ $alpha$ï¼å®ƒæ˜¯å¯ä¼˜åŒ–å‚æ•°ã€‚
  - $alpha = 0$ï¼š$Y gt.eq 0$
  - $alpha = 1$ï¼š$Y gt.eq X$
  - ä¸­é—´å€¼ï¼šç²¾åº¦ä¸é€Ÿåº¦çš„ trade-off
]
  ],
)





=== Back-Substitution

æ ¸å¿ƒç®—æ³•ï¼šé€’å½’å±•å¼€çº¿æ€§çº¦æŸç›´åˆ°è¾“å…¥å±‚ï¼Œè·å¾—æ›´ tight çš„ boundsã€‚

#algorithm(title: "Back-Substitution")[
  è®¡ç®—ç¥ç»å…ƒ $X_j$ çš„å…·ä½“ bounds $[L_j, U_j]$ï¼š

  1. è·å– $X_j$ çš„çº¿æ€§ä¸Šç•Œï¼š$X_j lt.eq sum_i c_i X_i + d$
  2. å¯¹æ¯ä¸ª $X_i$ï¼Œç”¨å…¶è‡ªèº«çš„çº¿æ€§çº¦æŸæ›¿æ¢
  3. é€’å½’ç›´åˆ°åˆ°è¾¾è¾“å…¥å±‚
  4. ç”¨è¾“å…¥ bounds è®¡ç®—æœ€ç»ˆæ•°å€¼
]

#tip[
  *ç¬¦å·åè½¬é™·é˜±*ï¼šè®¡ç®— upper bound æ—¶ï¼Œè‹¥ç³»æ•°ä¸º*è´Ÿ*ï¼Œéœ€è¦ç”¨å˜é‡çš„ *lower* boundï¼š

  $ "If" quad X_j lt.eq -X_i + dots, quad "then substitute" quad X_i gt.eq dots "(lower bound)" $

  åŸå› ï¼š$-X_i$ è¦æœ€å¤§åŒ–ï¼Œéœ€è¦ $X_i$ æœ€å°åŒ–ã€‚
]



#grid(
  columns: (1fr,1fr),
  [
  === Single-Neuron vs Multi-Neuron
  #figure(
  table(
    columns: 4,
    inset: 3pt,
    align: left,
    [*ç±»å‹*], [*ä¾èµ–å…³ç³»*], [*å¹¶è¡Œæ€§*], [*ç²¾åº¦*],
    [Single-Neuron], [ä»…ä¾èµ–å‰ä¸€å±‚], [å®Œå…¨å¹¶è¡Œï¼ˆGPU å‹å¥½ï¼‰], [è¾ƒä½],
    [Multi-Neuron], [å¯ç”¨åŒå±‚ç¥ç»å…ƒçº¦æŸ], [ä¸²è¡Œ], [è¾ƒé«˜],
  ),
  caption: [Relaxation ç±»å‹å¯¹æ¯”],
)
DeepPoly é‡‡ç”¨ Single-Neuronï¼Œç‰ºç‰²ç²¾åº¦æ¢å–å¹¶è¡Œæ€§ã€‚
],
  [
    

=== Triangle Relaxation ä¸ºä»€ä¹ˆä¸ Scale

```
Triangle (3 ä¸ªçº¦æŸ):          DeepPoly (2 ä¸ªçº¦æŸ):
        Y                              Y
        â”‚    /â”‚                        â”‚    /
        â”‚   / â”‚                        â”‚   /â–ˆâ–ˆâ–ˆ
        â”‚  /  â”‚                        â”‚  /â–ˆâ–ˆâ–ˆâ–ˆ
        â”‚ /   â”‚ â† Y â‰¥ X               â”‚ /â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
        â”‚/    â”‚                        â”‚/â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€ X              â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€ X
      L â”‚     U                      L â”‚     U

   çº¦æŸæ•°éšå±‚æ•°æŒ‡æ•°å¢é•¿           çº¦æŸæ•°å›ºå®šä¸º 2
```
  ],
)



== $alpha$-$beta$-CROWN

#grid(
  columns: (1fr, 1fr),[
    === Lagrange Multiplier æ–¹æ³•

é—®é¢˜ï¼šæœ´ç´  split ä¼šä¸¢å¤±å…³ç³»ä¿¡æ¯ï¼ˆDeepPoly åªèƒ½ç»´æŠ¤ä¸€å¯¹çº¦æŸï¼‰ã€‚

å¯¹äº split çº¦æŸ $X gt.eq 0$ï¼ˆå³ $-X lt.eq 0$ï¼‰ï¼š

$ max_X f(X) quad "s.t." quad X gt.eq 0 $

è½¬åŒ–ä¸º Lagrangianï¼š

$ max_X min_(beta gt.eq 0) [f(X) + beta dot X] $

ç”± *Weak Duality*ï¼š

$ max_X min_beta [dots] lt.eq min_beta max_X [dots] $

å³ä¾§æ›´å¥½å¤„ç†ï¼š
- $max_X$ å¯é€šè¿‡ back-substitution è®¡ç®—
- $min_beta$ å¯ç”¨gradientä¸‹é™ä¼˜åŒ–
  ],
  [#definition(title: "$alpha$-$beta$-CROWN ç»„æˆ")[
  - $alpha$ï¼šReLU ä¸‹ç•Œæ–œç‡å‚æ•°ï¼Œ$in [0, 1]$ï¼Œå¯gradientä¼˜åŒ–
  - $beta$ï¼šLagrange ä¹˜å­ï¼Œ$gt.eq 0$ï¼Œç”¨äºç¼–ç  split çº¦æŸ
  - CROWNï¼šDeepPoly æ¡†æ¶
]

#note[
  å…³é”®æ€§è´¨ï¼š$alpha$ å’Œ $beta$ åªå½±å“ *tightness*ï¼Œä¸å½±å“ *soundness*ã€‚
  - ä»»æ„ $alpha in [0, 1]$ éƒ½æ˜¯ sound çš„ ReLU relaxation
  - ä»»æ„ $beta gt.eq 0$ éƒ½ç»™å‡º sound çš„ upper bound
]]
)


== Floating-Point Soundness

#tip[
  å¾ˆå¤š"Sound" verifier åœ¨æµ®ç‚¹è¿ç®—ä¸‹å®é™…æ˜¯ *Unsound* çš„ï¼

  - ç†è®ºï¼šMILP åœ¨å®æ•° $RR$ ä¸Šæ˜¯ Sound & Complete
  - ç°å®ï¼šç¡¬ä»¶ç”¨ IEEE-754 æµ®ç‚¹æ•°ï¼Œæœ‰ rounding error

  $ "Sound"_("theory") eq.not "Sound"_("hardware") $
]

== Branch & Bound ç®—æ³•

#definition(title: "Branch & Bound for NN Verification")[
  *æ ¸å¿ƒæ€æƒ³*ï¼šç»“åˆ Relaxationï¼ˆå¿«ä½† incompleteï¼‰å’Œ Splittingï¼ˆæ…¢ä½† completeï¼‰

  *ç®—æ³•æµç¨‹*ï¼š
  1. ç”¨ DeepPoly/CROWN è®¡ç®— boundsï¼ˆ*Bound* é˜¶æ®µï¼‰
  2. è‹¥è¯æ˜æˆåŠŸ â†’ è¿”å› SAFE
  3. è‹¥ bounds ä¸å¤Ÿç´§ â†’ é€‰æ‹©ä¸€ä¸ª unstable ReLU splitï¼ˆ*Branch* é˜¶æ®µï¼‰
  4. é€’å½’å¤„ç†ä¸¤ä¸ªå­é—®é¢˜ï¼ˆ$X gt.eq 0$ å’Œ $X lt 0$ï¼‰
]

#grid(
  columns: (1fr, 1fr),
  [=== ç®—æ³•ä¼ªä»£ç 

    #algorithm(title: "Branch & Bound")[
      ```python
      def verify(spec, model, bounds):
          # 1. Bound: å°è¯•ç”¨ relaxation è¯æ˜
          lb, ub = compute_bounds(model, bounds)  # DeepPoly/CROWN

          if lb > 0:  # æ‰€æœ‰è¾“å‡º > 0
              return SAFE
          if ub < 0:  # å­˜åœ¨å¿…ç„¶è¿å
              return UNSAFE (with counterexample)

          # 2. Branch: é€‰æ‹©æœ€ä¸ç¨³å®šçš„ ReLU
          neuron = select_unstable_relu(bounds)  # å¯å‘å¼é€‰æ‹©

          # 3. é€’å½’
          result_pos = verify(spec, model, bounds âˆª {neuron â‰¥ 0})
          if result_pos == UNSAFE:
              return UNSAFE

          result_neg = verify(spec, model, bounds âˆª {neuron < 0})
          return result_neg
      ```
    ]],
  [
    === Branching å¯å‘å¼

    #figure(
      table(
        columns: 3,
        align: left,
        [*å¯å‘å¼*], [*é€‰æ‹©æ ‡å‡†*], [*ç›´è§‰*],
        [Largest Interval], [$max(u - l)$], [åŒºé—´æœ€å¤§çš„æœ€ä¸ç¡®å®š],
        [Closest to Zero], [$min(|l|, |u|)$], [æœ€æ¥è¿‘ 0 çš„æœ€å…³é”®],
        [Gradient-based], [$max |nabla_x "objective"|$], [å¯¹ç›®æ ‡å½±å“æœ€å¤§],
        [Learning-based], [ç¥ç»ç½‘ç»œé¢„æµ‹], [ä»å†å²å­¦ä¹ ],
      ),
    )

    === å¤æ‚åº¦åˆ†æ

    #theorem(title: "Branch & Bound å¤æ‚åº¦")[
      *æœ€åæƒ…å†µ*ï¼š$O(2^k)$ï¼Œå…¶ä¸­ $k$ æ˜¯ unstable ReLU æ•°é‡

      *å®é™…è¡¨ç°*ï¼šå–å†³äº
      - Bounds çš„ tightnessï¼ˆè¶Šç´§éœ€è¦ branch è¶Šå°‘ï¼‰
      - Branching å¯å‘å¼çš„è´¨é‡
      - é—®é¢˜æœ¬èº«çš„ç»“æ„

      *å…³é”®ä¼˜åŒ–*ï¼šç”¨ Î±-Î²-CROWN åœ¨ runtime ä¼˜åŒ– boundsï¼Œå‡å°‘ branch æ¬¡æ•°
    ]
  ],
)

== VNN-COMP ç«èµ›æ‰¹åˆ¤åˆ†æ

#tip[
  *è¯»è®ºæ–‡æ—¶å¿…é¡»è­¦æƒ•çš„æŒ‡æ ‡é™·é˜±ï¼*

  è®ºæ–‡å£°ç§° "éªŒè¯äº† 68,000,000 å‚æ•°ç½‘ç»œ" æ—¶ï¼Œç«‹å³æ£€æŸ¥ï¼š
]

#grid(
  columns: (1fr, 1fr, 1.2fr),
  gutter: 1em,
  [
    === éœ€è¦æ£€æŸ¥çš„æŒ‡æ ‡

    1. *Crossing ReLU æœ‰å¤šå°‘ï¼Ÿ*
      - è‹¥åªæœ‰ 10 ä¸ª â†’ $2^10 = 1024$
      - å¤æ‚åº¦å–å†³äº crossing æ•°è€Œéæ€»å‚æ•°

    2. *ç½‘ç»œå‡†ç¡®ç‡æ˜¯å¤šå°‘ï¼Ÿ*
      - è¿‡åº¦æ­£åˆ™åŒ–çš„ç½‘ç»œå®¹æ˜“éªŒè¯
      - ä½†å®é™… accuracy å¯èƒ½å¾ˆä½

    3. *ç”¨äº†ä»€ä¹ˆ specificationï¼Ÿ*
      - å° $epsilon$ â†’ æ›´å°‘ crossing
      - $epsilon = 0.001$ æ¯” $epsilon = 0.3$ éªŒè¯ç®€å•å¾—å¤š
  ],
  [
    === Critical Thinking

    #note[
      *Sound but Impractical*ï¼š
      - è®ºæ–‡å¯èƒ½åªéªŒè¯äº†ç‰¹æ®Šç½‘ç»œ
      - åœ¨ standard benchmark ä¸Šå¯èƒ½å¤±è´¥

      *Complete but Slow*ï¼š
      - MILP ç†è®ºä¸Š complete
      - ä½† timeout = 3600s åä¹Ÿç®—"è¯æ˜å¤±è´¥"
    ]

    $ "Verified" eq.not "Practically Robust" $
  ],
  [=== VNN-COMP å¸¸è§é—®é¢˜

    #figure(
      table(
        columns: 2,
        inset: 3pt,
        align: horizon,
        stroke: 0.75pt,
        [*é—®é¢˜*], [*è­¦ç¤º*],
        [Network å¤ªå°], [åªåœ¨ tiny network ä¸ŠéªŒè¯ï¼Œæ— æ³•æ³›åŒ–],
        [Epsilon å¤ªå°], [$epsilon = 2/255$ å¯¹ ImageNet å‡ ä¹æ²¡æ„ä¹‰],
        [Timeout å¤ªé•¿], [3600s è¯æ˜ä¸€ä¸ªsampleä¸å®ç”¨],
        [Certified Accuracy ä½], [éªŒè¯æˆåŠŸä½†åªæœ‰ 30% certified],
      ),
    )],
)



== Part 2 æ˜“é”™ç‚¹è¡¥å……

*Î± åœ¨ Î±-Î²-CROWN ä¸­çš„ä½œç”¨*ï¼šæ§åˆ¶ ReLU ä¸‹ç•Œæ–œç‡ï¼Œ$alpha in [0, 1]$ï¼Œå¯gradientä¼˜åŒ–

*Î² çš„ç‰©ç†æ„ä¹‰*ï¼šLagrange ä¹˜å­ï¼Œç¼–ç åˆ†æ”¯çº¦æŸ $X gt.eq 0$

*Weak Duality*ï¼š$max min lt.eq min max$ï¼ˆæ€»æ˜¯æˆç«‹ï¼‰

*ä¸ºä»€ä¹ˆ tighter relaxation ä¸ä¸€å®šæ›´å¥½?*ï¼šæ›´ç´§ = æ›´éš¾ä¼˜åŒ– = è®­ç»ƒå¯èƒ½å¤±è´¥

*Certified Accuracy çš„å±€é™*ï¼šåªè¡¡é‡"èƒ½è¯æ˜å®‰å…¨"çš„æ¯”ä¾‹ï¼Œä¸æ˜¯"å®é™…å®‰å…¨"çš„æ¯”ä¾‹

*Branch & Bound çš„ bottleneck*ï¼šä¸æ˜¯ç¥ç»å…ƒæ€»æ•°ï¼Œè€Œæ˜¯ *unstable ReLU æ•°é‡*



== ä¸‰ç§æ”»å‡»æ–¹æ³•å¯¹æ¯”

#figure(
  table(
    columns: 5,
    align: center,
    fill: (x, y) => if y == 0 { c1 },
    [*æ–¹æ³•*], [*æ­¥æ•°*], [*èŒƒæ•°çº¦æŸ*], [*ä¼˜åŒ–ç›®æ ‡*], [*å…¸å‹ç”¨é€”*],
    [FGSM], [1], [$ell_infinity$ï¼ˆå›ºå®šï¼‰], [å¿«é€Ÿå¯å‘å¼], [å¿«é€Ÿè¯„ä¼°è„†å¼±æ€§],
    [C&W], [å¤šæ­¥ä¼˜åŒ–], [$ell_2$ï¼ˆæœ€å°åŒ–ï¼‰], [æœ€å°æ‰°åŠ¨], [ç²¾ç¡®æ”»å‡»],
    [PGD], [10-20], [$ell_infinity$ï¼ˆæŠ•å½±ï¼‰], [æœ€å¤§åŒ– loss], [æ”»å‡» + Adversarial Training],
  ),
  caption: [ä¸‰ç§æ”»å‡»æ–¹æ³•å¯¹æ¯”],
)

#note[
  æ ¸å¿ƒå…³ç³»ï¼š$"PGD" = "FGSM" times K "è¿­ä»£" + "æŠ•å½±"$

  C&W ä»£è¡¨å¦ä¸€ç§å“²å­¦ï¼šæœ€å°åŒ–æ‰°åŠ¨å¤§å°ï¼Œè€Œéå›ºå®šæ‰°åŠ¨é¢„ç®—ã€‚
]

== Targeted vs Untargeted Attack

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === Targeted Attack

    ç›®æ ‡ï¼šä½¿modelè¾“å‡º*ç‰¹å®š*é”™è¯¯ç±»åˆ« $t eq.not y$

    $ eta^* = arg min_eta norm(eta)_p quad "s.t." quad f(x + eta) = t $

    ä¼˜åŒ–æ–¹å‘ï¼š*é è¿‘*ç›®æ ‡ç±»ï¼ˆgradientä¸‹é™ï¼‰
  ],
  [
    === Untargeted Attack

    ç›®æ ‡ï¼šä½¿modelè¾“å‡º*ä»»æ„*é”™è¯¯ç±»åˆ«

    $ eta^* = arg min_eta norm(eta)_p quad "s.t." quad f(x + eta) eq.not y $

    ä¼˜åŒ–æ–¹å‘ï¼š*è¿œç¦»*æ­£ç¡®ç±»ï¼ˆgradientä¸Šå‡ï¼‰
  ],
)

== FGSM

#definition(title: "Fast Gradient Sign Method")[
  #grid(
    columns: (1fr, 1fr),
    [*Targeted*ï¼š
      $ x' = x - epsilon dot "sign"(nabla_x cal(L)(f(x), t)) $],
    [
      *Untargeted*ï¼š
      $ x' = x + epsilon dot "sign"(nabla_x cal(L)(f(x), y)) $
    ],
  )

]

=== ä¸ºä»€ä¹ˆç”¨ Sign å‡½æ•°ï¼Ÿ

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    1. *å½’ä¸€åŒ–æ•ˆåº”*ï¼šgradientå„dimé‡çº§å·®å¼‚å·¨å¤§ï¼Œsign ç»Ÿä¸€ä¸º ${-1, 0, +1}$

    2. *æœ€å¤§æ­¥é•¿*ï¼šåœ¨ $ell_infinity$ çº¦æŸä¸‹ï¼Œæ¯ä¸ªåƒç´ éƒ½èµ°åˆ° box è¾¹ç•Œ

    3. *å•æ­¥æœ€ä¼˜*ï¼šä¸€é˜¶ Taylor å±•å¼€ä¸‹ï¼Œè¿™æ˜¯ $ell_infinity$ çº¦æŸçš„æœ€ä¼˜å•æ­¥ç§»åŠ¨
  ],
  [
    ```
    gradientç©ºé—´              Sign ç©ºé—´
    [100, 0.01, -50]  â†’  [1, 1, -1]
    ä¸åŒå°ºåº¦          â†’  ç»Ÿä¸€æ­¥é•¿ Îµ
    è¿ç»­å®æ•°å‘é‡      â†’  ç¦»æ•£æ–¹å‘é›†

    å‡ ä½•æ„ä¹‰ï¼šè·³åˆ° â„“âˆ çƒçš„é¡¶ç‚¹
    ```
  ],
)

== C&W Attack

#definition(title: "Carlini & Wagner")[
  #grid(
    columns: (1fr, 1fr),
    gutter: 1em,
    [*åŸé—®é¢˜*ï¼ˆéš¾ä¼˜åŒ–ï¼‰ï¼š
      $ min_eta norm(eta)_p quad "s.t." quad f(x + eta) = t $],
    [
      *æ¾å¼›å*ï¼ˆè¿ç»­å¯ä¼˜åŒ–ï¼‰ï¼š
      $ min_eta norm(eta)_p^2 + c dot "OPS"(x + eta, t) $

      å…¶ä¸­ $"OPS"(x', t) = max(0, max_(i eq.not t) Z(x')_i - Z(x')_t + kappa)$
    ],
  )
]


#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === OPS å‡½æ•°çš„å¥‘çº¦

    $ "OPS"(x', t) lt.eq 0 quad arrow.r.long quad f(x') = t $

    è¿™æ˜¯*å•å‘è•´å«*ï¼ˆSound but Incompleteï¼‰ï¼š
    - OPS $lt.eq 0$ â†’ æ”»å‡»å¿…ç„¶æˆåŠŸ
    - OPS $> 0$ â†’ ä¸ä¸€å®šå¤±è´¥
  ],
  [
    #note[
      å‚æ•° $kappa$ï¼ˆmarginï¼‰æ§åˆ¶ confidenceï¼š
      - $kappa = 0$ï¼šåªè¦åˆ†ç±»æ­£ç¡®å³å¯
      - $kappa > 0$ï¼šç›®æ ‡ç±» logit è‡³å°‘æ¯”å…¶ä»–ç±»å¤§ $kappa$
    ]
  ],
)



== PGD

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    #algorithm(title: "Projected Gradient Descent")[
      ```python
      åˆå§‹åŒ–: xâ‚€ = x + random_in_Îµ_box
      for k = 1 to K:
          # 1. FGSM step
          g_k = âˆ‡_x L(f(x_{k-1}), y)
          x'_k = x_{k-1} + Î± Â· sign(g_k)

          # 2. Projection (å…³é”®!)
          x_k = Î _{B_Îµ(x)}(x'_k)
      return x_K
      ```
    ]
  ],
  [
    å¯¹äº $ell_infinity$ èŒƒæ•°ï¼ŒæŠ•å½±æ“ä½œéå¸¸ç®€å•ï¼š
    $ Pi_(cal(B)_epsilon (x))(z) = "clip"(z, x - epsilon, x + epsilon) $

    é€dimè£å‰ªï¼šè¶…å‡ºèŒƒå›´åˆ™æ‹‰å›è¾¹ç•Œã€‚

    #tip[
      æŠ•å½±å¤æ‚åº¦æ˜¯å…³é”®ï¼š
      - $ell_infinity$ çƒï¼š$O(n)$ï¼ˆé€dim clipï¼‰
      - $ell_2$ çƒï¼š$O(n)$ï¼ˆå½’ä¸€åŒ–ï¼‰
      - å¤æ‚å‡¸å¤šé¢ä½“ï¼šå¯èƒ½éœ€è¦ QP solver

      è¿™æ˜¯ Certified Training çš„è®¡ç®—ç“¶é¢ˆä¹‹ä¸€ã€‚
    ]

  ],
)


== Adversarial Training

#grid(
  columns: (1fr, 1fr),
  [#definition(title: "Adversarial Training (PGD-AT)")[
    $ min_theta bb(E)_((x, y) tilde cal(D)) [max_(delta in cal(B)_epsilon (0)) cal(L)(f_theta (x + delta), y)] $

    - *Inner max*ï¼šç”¨ PGD æ‰¾ worst-case å¯¹æŠ—sample
    - *Outer min*ï¼šåœ¨å¯¹æŠ—sampleä¸Šåšæ ‡å‡† SGD
  ]],
  [=== ä¼ªä»£ç 

    ```python
    for (x, y) in train_loader:
        # Inner Max: æ‰¾æœ€éš¾çš„å¯¹æŠ—sample
        x_adv = PGD_attack(x, model, epsilon, steps=10)

        # Outer Min: åœ¨å¯¹æŠ—sampleä¸Šè®­ç»ƒ
        loss = CrossEntropy(model(x_adv), y)
        loss.backward()  # å¯¹ Î¸ æ±‚gradient
        optimizer.step()
    ```],
)





=== Contrastive Learning è§†è§’

#note[
  Adversarial Training å¯ç†è§£ä¸ºå¯¹æŠ—æ€§å¯¹æ¯”å­¦ä¹ ï¼š
  - *Anchor*ï¼šåŸå§‹è¾“å…¥ $x$
  - *Positive Bag*ï¼š$cal(B)_epsilon (x)$ å†…æ‰€æœ‰ç‚¹ï¼ˆè¯­ä¹‰åº”ä¿æŒä¸å˜ï¼‰
  - *Hard Negative*ï¼šPGD åœ¨ $cal(B)_epsilon (x)$ å†…æ‰¾åˆ°çš„æœ€å¤§ loss ç‚¹

  ä¼ ç»Ÿå¯¹æ¯”å­¦ä¹ é‡‡æ ·*æœ‰é™ä¸ª*è´Ÿsampleï¼›Adversarial Training å¯¹*æ•´ä¸ª $epsilon$-çƒ*éƒ½é²æ£’ã€‚
]

== TRADESï¼šç²¾åº¦-é²æ£’æ€§æƒè¡¡

#definition(title: "TRADES Loss")[
  $
    cal(L)_("TRADES") = underbrace(cal(L)(f(x), y), "Natural Accuracy") + lambda underbrace(max_(x' in cal(B)_epsilon) "KL"(f(x) || f(x')), "Robustness")
  $

  *æ ¸å¿ƒæ€æƒ³*ï¼šè‡ªç„¶å‡†ç¡®ç‡å’Œé²æ£’æ€§*åˆ†å¼€*ä¼˜åŒ–ï¼Œç”¨ $lambda$ æƒè¡¡ã€‚
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === PGD-AT vs TRADES

    #figure(
      table(
        columns: 3,
        align: left,
        [], [*PGD-AT*], [*TRADES*],
        [Loss], [$max cal(L)(f(x'), y)$], [$cal(L)(f(x), y) + lambda "KL"$],
        [ç›®æ ‡], [æœ€å°åŒ–æœ€åæƒ…å†µ], [å¹³è¡¡ç²¾åº¦ä¸é²æ£’æ€§],
        [Clean Acc], [è¾ƒä½], [è¾ƒé«˜],
        [Robust Acc], [è¾ƒé«˜], [ä¸­ç­‰],
      ),
    )
  ],
  [
    === é€‰æ‹©æŒ‡å—

    - *éœ€è¦æœ€é«˜é²æ£’æ€§*ï¼šä½¿ç”¨ PGD-AT
    - *éœ€è¦å¹³è¡¡*ï¼šä½¿ç”¨ TRADES + è°ƒèŠ‚ $lambda$
    - $lambda$ è¶Šå¤§ â†’ è¶Šå…³æ³¨é²æ£’æ€§
    - å…¸å‹å€¼ï¼š$lambda in [1, 6]$

    #note[
      å®è·µä¸­ TRADES é€šå¸¸åœ¨ clean-robust æƒè¡¡æ›²çº¿ä¸Šè¡¨ç°æ›´å¥½ã€‚
    ]
  ],
)

#grid(
  columns: (1fr, 1fr),
  [== AutoAttackï¼šå¯é çš„æ”»å‡»è¯„ä¼°

    #definition(title: "AutoAttack Ensemble")[
      *ç»„æˆ*#footnote[è®¾è®¡æ€æƒ³ï¼šç»„åˆå¤šç§æ”»å‡»ä»¥é¿å…å‡é˜³æ€§ï¼ˆè¯¯ä»¥ä¸ºmodelé²æ£’ï¼‰]ï¼š
      1. *APGD-CE*ï¼šAuto-PGD with CE loss
      2. *APGD-DLR*ï¼šAuto-PGD with Difference of Logits Ratio loss
      3. *FAB*ï¼šFast Adaptive Boundary attack
      4. *Square Attack*ï¼šBlack-box query-based attack
    ]],
  [#tip[
    *ä¸ºä»€ä¹ˆéœ€è¦ AutoAttackï¼Ÿ*
    - å•ä¸€æ”»å‡»å¯èƒ½è¢«"è¿‡æ‹Ÿåˆé˜²å¾¡"ç»•è¿‡
    - è®ºæ–‡å¯èƒ½é€‰æ‹©æ€§æŠ¥å‘Šå¼±æ”»å‡»ç»“æœ
    - AutoAttack æä¾›*æ ‡å‡†åŒ–è¯„ä¼°*

    *ä½¿ç”¨è§„åˆ™*ï¼š
    - æŠ¥å‘Š Robust Accuracy æ—¶å¿…é¡»ç”¨ AutoAttack
    - è‡ªå®šä¹‰æ”»å‡»ç»“æœåªèƒ½ä½œä¸º*è¡¥å……*
  ]],
)



= Part 4: Certified Training <sec:certified>

#grid(
  columns: (1fr, 1fr),
  [
    == PGD Training vs Certified Training

    #figure(
      table(
        columns: 3,
        align: left,
        [], [*PGD Training*], [*Certified Training*],
        [ä¼˜åŒ–ç©ºé—´], [è¾“å…¥ç©ºé—´ $S(x)$], [è¾“å‡ºç©ºé—´ $gamma(f^sharp (S(x)))$],
        [Inner max], [$max_(x' in S(x)) cal(L)(f(x'), y)$], [$max_(z in gamma(f^sharp (S(x)))) cal(L)(z, y)$],
        [ä½¿ç”¨çš„ç‚¹], [å…·ä½“å¯¹æŠ—sample], [ç¬¦å·åŒºåŸŸï¼ˆå« garbage pointsï¼‰],
        [ä¿è¯ç±»å‹], [Heuristicï¼ˆå¯èƒ½ miss attacksï¼‰], [Soundï¼ˆå¯è¯æ˜ä¿è¯ï¼‰],
        [è®¡ç®—æ–¹å¼], [å…·ä½“å‰å‘ä¼ æ’­], [ç¬¦å·å‰å‘ä¼ æ’­],
      ),
      caption: [è®­ç»ƒèŒƒå¼å¯¹æ¯”],
    )

    ```
    PGD Training:                     Certified Training:

       S(x)                              S(x)
        â—                                 â—
       â•± â•²                               â•± â•²
      â•±   â•²     æ”»å‡»ç©ºé—´                â•±   â•²
     â—â”€â”€â”€â”€â”€â—                           â—â”€â”€â”€â”€â”€â—
        â†“                                  â†“
        â†“  æ‰¾ worst-case è¾“å…¥             â†“  Convex Propagation
        â†“                                  â†“
       â—â”€â”€â—                            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â† è¾“å‡º region
      å…·ä½“è¾“å‡º                           (å« garbage points)
                                            â†“
                                       åœ¨è¿™é‡Œæ‰¾ worst-case
    ```
  ],
  [
    == Certified Training Paradox

    å®éªŒå‘ç°ï¼š*æ›´ tight çš„ relaxation åè€Œå¯¼è‡´æ›´å·®çš„è®­ç»ƒç»“æœï¼*

    #figure(
      table(
        columns: 3,
        align: center,
        [*Relaxation*], [*Tightness*], [*Certified Accuracy*],
        [Box (IBP)], [Low], [86%],
        [Zonotope], [Medium], [73%],
        [DeepPoly], [High], [70%],
      ),
      caption: [Tightness vs Training Performanceï¼ˆåç›´è§‰ï¼ï¼‰],
    )

    === åŸå› åˆ†æ

    #grid(
      columns: (1fr, 1fr),
      gutter: 1em,
      [
        *Sensitivityï¼ˆæ•æ„Ÿæ€§ï¼‰*ï¼š
        - DeepPoly æœ‰ discrete switchingï¼ˆé€‰ $alpha$ æ—¶ï¼‰
        - æƒé‡å°å˜åŒ– â†’ bounds å‰§å˜
        - gradientä¸ç¨³å®š
      ],
      [
        *Discontinuityï¼ˆä¸è¿ç»­æ€§ï¼‰*ï¼š
        - å¤æ‚ relaxation å¼•å…¥æ›´å¤šä¸è¿ç»­ç‚¹
        - ä¼˜åŒ– landscape æ›´éš¾ navigate
      ],
    )

    ```
    Box çš„ä¼˜åŒ– landscape:          DeepPoly çš„ä¼˜åŒ– landscape:

        â•²    â•±                        â•±â•²  â•±â•²  â•±â•²
         â•²  â•±                        â•±  â•²â•±  â•²â•±  â•²
          â•²â•±                        â•±           â•²
        smooth!                     discontinuous!
    ```

    #note[
      åç›´è§‰ä½†é‡è¦ï¼š*Tightness $eq.not$ Optimizability*

      Box è™½ç„¶æ¾ï¼ˆç²¾åº¦ä½ï¼‰ï¼Œä½†gradientå¹³æ»‘ï¼Œåè€Œæ›´å¥½ä¼˜åŒ–ã€‚
    ]
  ],
)



== SABR: Layer-wise Training

æ ¸å¿ƒæ€æƒ³ï¼šåœ¨ä¸­é—´å±‚åš PGDï¼Œè€Œéåœ¨è¾“å‡ºç©ºé—´ä¼˜åŒ–ã€‚

#algorithm(title: "SABR Method")[
  å¯¹äºæ¯å±‚ $k$ï¼š
  1. ç”¨ convex relaxation å°†è¾“å…¥ spec ä¼ æ’­åˆ°ç¬¬ $k$ å±‚
  2. å†»ç»“ç¬¬ $k$ å±‚ä¹‹å‰çš„å‚æ•°
  3. åœ¨ intermediate shape ä¸Šåš PGDï¼ˆåªè®­ç»ƒåé¢çš„å±‚ï¼‰
  4. æ›´æ–°ç¬¬ $k$ å±‚åŠä¹‹åçš„æƒé‡
]

```
Input     Layer 1    Layer 2    Layer 3    Output
  â—â”€â”€â”€â”€â”€â”€â”€â”€Hâ‚â”€â”€â”€â”€â”€â”€â”€â”€Hâ‚‚â”€â”€â”€â”€â”€â”€â”€â”€Hâ‚ƒâ”€â”€â”€â”€â”€â”€â”€â”€â—
  â”‚                  â”‚                    â”‚
  â”‚         â‘         â”‚         â‘¡          â”‚
  â”‚    Propagate     â”‚     PGD here!      â”‚
  â”‚    (frozen)      â”‚    (train Hâ‚‚,Hâ‚ƒ)   â”‚
  â†“                  â†“                    â†“
 S(x) â”€â”€convexâ”€â”€â†’ Shape â”€â”€PGDâ”€â”€â†’ worst points
```

#tip[
  æŠ•å½±é—®é¢˜ï¼šPGD éœ€è¦æŠ•å½±åˆ° $S(x)$ï¼Œä½†ä¸­é—´å±‚ shape å¯èƒ½ä¸æ˜¯ $ell_infinity$ çƒï¼

  - $ell_infinity$ ball æŠ•å½±ï¼šç®€å• clip
  - DeepPoly shape æŠ•å½±ï¼šéœ€è¦è§£ QP

  è§£å†³æ–¹æ¡ˆï¼šç”¨ Zonotopeï¼ˆå¯é«˜æ•ˆæŠ•å½±ï¼‰ã€‚
]

== Logic â†’ Loss Translation

#grid(
  columns: (2fr, 1fr),
  [#definition(title: "é€»è¾‘çº¦æŸåˆ°losså‡½æ•°")[
      ä»»æ„é€»è¾‘å…¬å¼ $phi$ å¯ç¿»è¯‘ä¸ºloss $L_phi$ï¼Œæ»¡è¶³ï¼š
      $ L_phi (x) = 0 quad arrow.l.r.double quad x tack.double phi $
    ]
    #note[
      è¿™æä¾›äº†å¤„ç†ä»»æ„ safety specs çš„ç»Ÿä¸€æ¡†æ¶ï¼š
      - Adversarial attackï¼š$exists delta: norm(delta)_infinity lt.eq epsilon and arg max f(x + delta) eq.not y$
      - Robustness verificationï¼š$forall delta: norm(delta)_infinity lt.eq epsilon arrow.r.double f(x + delta) = y$
      - è®­ç»ƒï¼š$min_theta max_(z in S(x)) L_(not phi)(z)$
    ]],
  [#figure(
    table(
      columns: 2,
      align: left,
      [*å…¬å¼ $phi$*], [*loss $L_phi$*],
      [$t_1 = t_2$], [$(t_1 - t_2)^2$],
      [$t_1 lt.eq t_2$], [$max(0, t_1 - t_2)^2$],
      [$phi_1 and phi_2$], [$L_(phi_1) + L_(phi_2)$],
      [$phi_1 or phi_2$], [$L_(phi_1) dot L_(phi_2)$],
    ),
    caption: [Logic â†’ Loss ç¿»è¯‘è¡¨],
  )],
)



= Part 5: Randomized Smoothing & GCG Attack <sec:rs-gcg>

== Randomized Smoothing

=== æ ¸å¿ƒæ€æƒ³



#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    #definition(title: "Smoothed Classifier")[
      ç»™å®š base classifier $F$ï¼ˆé»‘ç›’ï¼‰ï¼Œæ„é€  smoothed classifier $G$ï¼š
      $ G(x) = arg max_c bb(P)_(epsilon tilde cal(N)(0, sigma^2 I))[F(x + epsilon) = c] $

      ç›´è§‰ï¼šå¯¹æ¯ä¸ªè¾“å…¥ï¼Œé‡‡æ ·å¤§é‡Gaussian noiseæ‰°åŠ¨ï¼Œç”¨ majority vote å†³å®šè¾“å‡ºã€‚
    ]
  ],
  [å…³é”®åŒºåˆ†ï¼š
    - *å®šç†ä¿è¯æ˜¯ deterministic*ï¼ˆæ•°å­¦è¯æ˜ï¼‰
    - *å®è·µä¼°è®¡æ˜¯ probabilistic*ï¼ˆé‡‡æ · Monte Carloï¼‰

    ä¸è¦æ··æ·†è¿™ä¸¤è€…ï¼
    ```
    Base Classifier Fï¼ˆå¯èƒ½è„†å¼±ï¼‰
             â†“
        ğŸ² Gaussian noiseåŒ…è£¹
             â†“
    Smoothed Classifier Gï¼ˆæ„é€ å‡ºé²æ£’æ€§ï¼‰
    ```
  ],
)

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === Certified Radius

    #theorem(title: "è®¤è¯åŠå¾„å…¬å¼")[
      è®¾ $underline(p_A)$ ä¸ºæœ€é«˜ç±»probçš„ä¸‹ç•Œï¼Œä¸” $underline(p_A) > 0.5$ï¼Œåˆ™ï¼š
      $ R = sigma dot Phi^(-1)(underline(p_A)) $

      å…¶ä¸­ $Phi^(-1)$ æ˜¯æ ‡å‡†æ­£æ€ CDF çš„é€†å‡½æ•°ï¼ˆprobit functionï¼‰ã€‚
    ]

    #tip[
      å¢å¤§ $sigma$ ä¸ä¸€å®šå¢å¤§ $R$ï¼
      - ç›´æ¥æ•ˆåº”ï¼š$sigma$ é¡¹å¢å¤§
      - é—´æ¥æ•ˆåº”ï¼š$p_A$ é™ä½ï¼ˆå™ªå£°å¤§ï¼Œåˆ†ç±»æ•£ä¹±ï¼‰

      å­˜åœ¨æœ€ä¼˜ $sigma^*$ï¼Œéœ€ empirical tuningã€‚
    ]
  ],
  [
    === ä¸¤é˜¶æ®µé‡‡æ ·

    #algorithm(title: "Certification Pipeline")[
      *Stage 1*ï¼ˆExplorationï¼Œ$n_0 approx 100$ï¼‰ï¼š
      ```python
      votes = [F(x + noise) for _ in range(n0)]
      c_A = most_common(votes)  # çŒœæµ‹ top class
      ```

      *Stage 2*ï¼ˆCertificationï¼Œ$n approx 10^5$ï¼‰ï¼š
      ```python
      votes = [F(x + noise) for _ in range(n)]
      p_A_hat = count(votes == c_A) / n
      p_A_lower = binomial_CI_lower(p_A_hat, n, Î±)
      if p_A_lower > 0.5:
          R = Ïƒ * Î¦â»Â¹(p_A_lower)
      else:
          return "Abstain"
      ```
    ]
  ],
)




=== Deterministic vs Randomized Smoothing

#figure(
  table(
    columns: 3,
    align: left,
    [], [*Deterministic (CROWN)*], [*Randomized Smoothing*],
    [ä¿è¯ç±»å‹], [100% ç¡®å®š], [$(1-alpha)$ confidence],
    [modelå‡è®¾], [éœ€çŸ¥é“æƒé‡ã€æ¿€æ´»å‡½æ•°], [é»‘ç›’å³å¯],
    [å¯æ‰©å±•æ€§], [å°ç½‘ç»œï¼ˆç²¾åº¦çˆ†ç‚¸ï¼‰], [ä»»æ„å¤§å°ï¼ˆåŒ…æ‹¬ LLMï¼‰],
    [èŒƒæ•°], [$ell_infinity, ell_1, ell_2$ çš†å¯], [ä¸»è¦ $ell_2$ï¼ˆå¯¹åº” Gaussianï¼‰],
  ),
  caption: [Certification æ–¹æ³•å¯¹æ¯”],
)

=== ä¸ºä»€ä¹ˆ RS ä¸»è¦é™äº $ell_2$ï¼Ÿ

#theorem(title: "Gaussian å™ªå£°ä¸ $ell_2$ çš„æ•°å­¦è”ç³»")[
  Gaussian åˆ†å¸ƒå…·æœ‰*æ—‹è½¬ä¸å˜æ€§*#footnote[æ•°å­¦ä¸Šï¼š$cal(N)(0, sigma^2 I)$ åœ¨æ­£äº¤å˜æ¢ä¸‹ä¸å˜]ï¼š
  $ X tilde cal(N)(0, sigma^2 I) arrow.r.double norm(X)_2 "ä¸æ–¹å‘æ— å…³" $

  è¿™å¯¼è‡´ Neyman-Pearson æœ€ä¼˜æ£€æµ‹å™¨åœ¨ $ell_2$ çƒä¸Šå‡åŒ€ï¼Œä»è€Œå¾—åˆ° $ell_2$ certified radiusã€‚

  *å…¶ä»–èŒƒæ•°çš„å›°éš¾*ï¼š
  - $ell_infinity$ï¼šéœ€è¦ discrete/uniform noiseï¼Œä½†probç•Œæ›´å¼±
  - $ell_1$ï¼šéœ€è¦ Laplace noiseï¼Œä½† certified radius å…¬å¼æ›´å¤æ‚
]

#tip[
  ä¸è¦ä¸ DP ä¸­çš„ Laplace vs Gaussian æ··æ·†ï¼
  - DP ä¸­ï¼šLaplace å¯¹åº” $ell_1$ *æ•æ„Ÿåº¦*ï¼ŒGaussian å¯¹åº” $ell_2$ *æ•æ„Ÿåº¦*
  - RS ä¸­ï¼šGaussian å¯¹åº” $ell_2$ *certified radius*
]

=== DP ä¸ RS çš„å¯¹å¶æ€§ï¼ˆProf å¼ºè°ƒï¼ï¼‰

#theorem(title: "åŒä¸€æšç¡¬å¸çš„ä¸¤é¢")[
  DP å’Œ RS ä½¿ç”¨*ç›¸åŒçš„æ•°å­¦å·¥å…·*ï¼ˆå™ªå£°æœºåˆ¶ã€æŒ‡æ•°ç•Œï¼‰ï¼Œä½†*ä¼˜åŒ–æ–¹å‘ç›¸å*ï¼š

  #figure(
    table(
      columns: 3,
      align: left,
      [], [*Differential Privacy*], [*Randomized Smoothing*],
      [ç›®æ ‡], [ä½¿åˆ†å¸ƒ*ä¸å¯åŒºåˆ†*], [ä½¿é¢„æµ‹*å¯åŒºåˆ†*],
      [æ•°å­¦], [$P[M(D)] approx P[M(D')]$], [$P[G(x)=c] gt.double P[G(x) eq.not c]$],
      [å™ªå£°ä½œç”¨], [æ··æ·†çœŸå®æ•°æ®], [å¹³æ»‘å†³ç­–è¾¹ç•Œ],
      [å‡è®¾æ£€éªŒ], [å¸Œæœ› Power *ä½*], [å¸Œæœ›ç½®ä¿¡åº¦ *é«˜*],
    ),
  )
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === å…±åŒçš„ Lipschitz åŸºç¡€

    ä¸¤è€…çš„è¯æ˜éƒ½ä¾èµ– Lipschitz å¸¸æ•° $L$ï¼š
    - DPï¼š$L$ æ§åˆ¶æ•æ„Ÿåº¦ â†’ å†³å®šå™ªå£°é‡
    - RSï¼š$L$ æ§åˆ¶ $p_A$ éš $x$ å˜åŒ– â†’ å†³å®šè®¤è¯åŠå¾„

    $ "DP Noise" prop frac(L, epsilon), quad "RS Radius" prop frac(sigma, L) $
  ],
  [
    === è€ƒè¯•å¸¸è€ƒå¯¹æ¯”

    #tip[
      *å¸¸è§é™·é˜±*ï¼š
      - è¯¯ä»¥ä¸º RS æ˜¯ probabilistic methodï¼ˆå®šç†æ˜¯ç¡®å®šæ€§çš„ï¼ï¼‰
      - æ··æ·† DP å’Œ RS çš„å™ªå£°å«ä¹‰
      - å¿˜è®° $ell_2$ é™åˆ¶çš„æ•°å­¦åŸå› 
    ]
  ],
)

=== å¸¸è§å¤±è´¥æ¨¡å¼

#figure(
  table(
    columns: 3,
    align: left,
    [*Case*], [*é—®é¢˜*], [*è§£å†³æ–¹æ¡ˆ*],
    [çŒœé”™ top class], [$n_0$ å¤ªå°], [å¢å¤§ $n_0$ï¼ˆ100-1000ï¼‰],
    [$p_A lt 0.5$], [Base model åœ¨å™ªå£°ä¸‹è¡¨ç°å·®], [Gaussian Adversarial Training],
    [Lower bound å¤ªæ¾], [çœŸå® $p_A = 0.52$ï¼Œä¼°è®¡ $underline(p_A) = 0.45$], [å¢å¤§ $n$ï¼ˆ10k â†’ 100kï¼‰],
  ),
)



=== æ ¸å¿ƒæŒ‘æˆ˜

LLM è¾“å…¥æ˜¯ discrete tokensï¼Œä¸èƒ½ç›´æ¥ç”¨ PGDï¼ˆéœ€è¿ç»­ç©ºé—´ï¼‰ã€‚

#definition(title: "GCG ä¼˜åŒ–ç›®æ ‡")[
  æ‰¾ suffix ä½¿modelç”Ÿæˆæœ‰å®³å†…å®¹ï¼š
  $ min_("suffix") cal(L)_("CE")(y_("target") = "Sure" | "prompt", "suffix") $
]

=== Three-Step Algorithm

#algorithm(title: "GCG Algorithm")[
  *Step 1*ï¼šOne-hot gradientè®¡ç®—ï¼ˆå…³é”® trickï¼‰
  - æŠŠ token å˜æˆ one-hot vector $e in RR^(|V|)$
  - è®¡ç®— $nabla_e cal(L)$ï¼ˆè¿ç»­ç©ºé—´ï¼‰

  *Step 2*ï¼šTop-K ç­›é€‰
  - é€‰gradientæœ€è´Ÿçš„ $K$ ä¸ª tokens ä½œä¸ºå€™é€‰
  - ä» 50k è¯è¡¨ç­›åˆ° ~256 ä¸ª

  *Step 3*ï¼šGreedy Search
  ```python
  for position i in suffix:
      for token in top_k_candidates:
          suffix[i] = token
          loss = evaluate(prompt + suffix)
          keep best
  ```
]

#tip[
  GCG ä¸æ˜¯åœ¨è¿ç»­ç©ºé—´æ›´æ–°ï¼Œè€Œæ˜¯ç”¨gradientä½œä¸ºå¯å‘å¼ç­›é€‰å€™é€‰ï¼Œå†å›åˆ°ç¦»æ•£ç©ºé—´åš greedy searchã€‚
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === White-box vs Black-box

    #figure(
      table(
        columns: 3,
        align: left,
        [], [*White-box GCG*], [*Black-box*],
        [gradient], [å¯ç”¨], [ä¸å¯ç”¨],
        [åˆå§‹åŒ–], [éšæœºå³å¯], [éœ€å¼ºåˆå§‹åŒ–ï¼ˆFIM Inversionï¼‰],
        [é€Ÿåº¦], [å¿«ï¼ˆTop-K ç­›é€‰ï¼‰], [æ…¢ï¼ˆç›²ç›®æœç´¢ï¼‰],
      ),
    )
  ],
  [Black-box ç­–ç•¥ï¼šç”¨ Fill-in-the-Middle modelåš inversion attackï¼Œå…ˆå†™æ¶æ„ä»£ç ï¼Œåæ¨ prefix ä½œä¸ºå¼ºåˆå§‹åŒ–ã€‚
    === Universal & Transferable Suffix

    $ min_("suffix") sum_(i=1)^M cal(L)("Sure" | "prompt"_i, "suffix") $

    åœ¨å¤šä¸ª prompts ä¸ŠåŒæ—¶ä¼˜åŒ– â†’ universal suffix â†’ å¯ transfer åˆ°å…¶ä»–modelï¼ˆç”šè‡³ GPT-4ï¼‰ã€‚

  ],
)




== Mixed Adversarial Training

=== åŠ¨æœº

#figure(
  table(
    columns: 4,
    align: left,
    [*Attack Type*], [*Speed*], [*Strength*], [*Realistic?*],
    [Continuous], [å¿«ï¼ˆgradientä¸‹é™ï¼‰], [ä¸­ç­‰], [å¦ï¼ˆtoken+0.1 æ— æ„ä¹‰ï¼‰],
    [Discrete (GCG)], [æ…¢ï¼ˆgreedy searchï¼‰], [å¼º], [æ˜¯ï¼ˆçœŸå®æ”»å‡»ï¼‰],
  ),
)

#definition(title: "Mixed-AT Loss")[
  $
    cal(L)_("total") = underbrace(cal(L)_("clean")(x, y), "ä¿æŒæ•ˆç”¨") + underbrace(cal(L)_("robust")(x_("adv"), y_("safe")), "é²æ£’æ€§") + underbrace(cal(L)_("refuse")(x_("adv"), y_("refuse")), "æ‹’ç»æ¶æ„")
  $
]

ç­–ç•¥ï¼š
1. Discrete attackï¼ˆGCGï¼‰ç”Ÿæˆå¼ºå¯¹æŠ—sampleä½œä¸º anchor
2. Continuous attack ç”Ÿæˆå¤§é‡å˜ç§æ‰©å……å¤šæ ·æ€§
3. ç»“åˆä¸¤è€…è®­ç»ƒ â†’ ASR ä» 50% é™åˆ° $lt$ 10%

== Post-Training Attacks

=== Quantization Attack

åˆ©ç”¨é‡åŒ–å‰åè¡Œä¸ºå·®å¼‚æ¤å…¥åé—¨ï¼š

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    *æœºåˆ¶*ï¼š
    1. è®­ç»ƒ malicious model
    2. è®¡ç®— Box Constraintï¼š$[w_("low"), w_("high")]$ ä½¿é‡åŒ–å€¼ä¸å˜
    3. åœ¨ box å†…ç”¨ clean data fine-tuneï¼Œä½¿ FP32 è¡¨ç°æ­£å¸¸
  ],
  [
    *ç»“æœ*ï¼š
    - FP32ï¼šbenignï¼ˆé€šè¿‡æ£€æµ‹ï¼‰
    - INT8ï¼šmaliciousï¼ˆé‡åŒ–åæ¿€æ´»ï¼‰

    Defense ç›²ç‚¹ï¼šæ£€æµ‹åœ¨ FP32ï¼Œéƒ¨ç½²ç”¨ INT8ã€‚
  ],
)

=== Fine-Tuning Attack

åˆ©ç”¨ Meta-Learning åœ¨ç”¨æˆ·å¾®è°ƒåæ¿€æ´»åé—¨ï¼š

$
  cal(L) = underbrace(cal(L)_("clean")(theta), "ç°åœ¨å®‰å…¨") + lambda underbrace(cal(L)_("attack")(theta - nabla cal(L)_("user")(theta)), "æœªæ¥æ¶æ„")
$

#tip[
  éœ€è¦äºŒé˜¶å¯¼æ•°ï¼ˆHessianï¼‰ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼š
  $
    frac(partial cal(L)(theta'), partial theta) = frac(partial cal(L), partial theta') dot (I - eta nabla^2 cal(L)_("user"))
  $
]

= Part 6: è€ƒè¯•è¦ç‚¹ <sec:exam>
== æ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥
#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === Sound vs Complete

    - *Sound*ï¼šè¯æ˜æˆç«‹ â†’ ç¡®å®æˆç«‹ï¼ˆä¸ä¼šè¯¯æŠ¥å®‰å…¨ï¼‰
    - *Complete*ï¼šç¡®å®æˆç«‹ â†’ èƒ½å¤Ÿè¯æ˜ï¼ˆä¸ä¼šæ¼æŠ¥å¯è¯æ€§è´¨ï¼‰
    - å¤§å¤šæ•°å®ç”¨æ–¹æ³•ï¼šSound but Incomplete

    === Crossing ReLU

    - å®šä¹‰ï¼šè¾“å…¥ bounds $l < 0 < u$
    - MILP å¤æ‚åº¦ï¼š$O(2^k)$ï¼Œ$k$ = Crossing ReLU æ•°é‡ï¼ˆéæ€»ç¥ç»å…ƒæ•°ï¼‰
    - å‡å°‘æ–¹æ³•ï¼šæ›´ tight çš„ boundsï¼ŒCertified Training
  ],
  [
    === Min-Max ç»“æ„

    $ min_theta max_(delta in Delta) cal(L)(f_theta (x + delta), y) $

    - Attackï¼šå›ºå®š $theta$ï¼Œæ‰¾ $delta$
    - Defenseï¼šåŒæ—¶ä¼˜åŒ–ä¸¤è€…
    - Certificationï¼šç”¨ convex relaxation æ›¿ä»£ inner max

    === å‚æ•°å«ä¹‰

    - $alpha$ï¼šReLU ä¸‹ç•Œæ–œç‡ï¼ˆ$in [0, 1]$ï¼‰ï¼Œå¯ä¼˜åŒ–
    - $beta$ï¼šLagrange ä¹˜å­ï¼ˆ$gt.eq 0$ï¼‰ï¼Œç¼–ç  split çº¦æŸ
    - ä¸¤è€…åªå½±å“ tightnessï¼Œä¸å½±å“ soundness
  ],
)

#grid(
  columns: (1fr, 1fr),
  gutter: (),
  [== æ–¹æ³•å¯¹æ¯”è¡¨

    #figure(
      table(
        columns: 5,
        align: center,
        [*æ–¹æ³•*], [*Sound*], [*Complete*], [*å¤æ‚åº¦*], [*GPU*],
        [Box/IBP], [âœ“], [âœ—], [$O(n)$], [âœ“],
        [DeepPoly], [âœ“], [âœ—], [$O(n^3 L^2)$], [âœ“],
        [MILP], [âœ“], [âœ“], [$O(2^k)$], [âœ—],
        [RS], [ç»Ÿè®¡], [â€”], [$O(n_("samples"))$], [âœ“],
      ),
    )],
  [== æ˜“é”™ç‚¹

    *Crossing ReLU æ•°é‡*ï¼šå¤æ‚åº¦å–å†³äº crossing neuronsï¼Œä¸æ˜¯æ€»ç¥ç»å…ƒæ•°

    *Back-substitution ç¬¦å·*ï¼šè´Ÿç³»æ•°éœ€è¦ç”¨ opposite bound

    *æµ®ç‚¹ soundness*ï¼š$"Sound"_("theory") eq.not "Sound"_("hardware")$

    *Training paradox*ï¼šæ›´ tight $eq.not$ æ›´å¥½ä¼˜åŒ–

    *RS ä¿è¯ç±»å‹*ï¼šå®šç†æ˜¯ deterministicï¼Œä¼°è®¡æ˜¯ probabilistic

    *å¢å¤§ $sigma$*ï¼šä¸ä¸€å®šå¢å¤§ $R$ï¼ˆ$p_A$ ä¼šä¸‹é™ï¼‰

    *GCG vs PGD*ï¼šGCG ç”¨gradientç­›é€‰ï¼Œä¸æ˜¯ç”¨gradientæ›´æ–°

    *$n_0$ vs $n$*ï¼šClassification ($n_0$) vs Estimation ($n$)ï¼Œä¿¡æ¯å¤æ‚åº¦ä¸åŒ],
)




= Part 7: Privacy & Differential Privacy <sec:privacy>

== Differential Privacy æ ¸å¿ƒæ€æƒ³

#grid(
  columns: (1.2fr, 1fr),
  gutter: 1em,
  [
    #definition(title: "DP çš„å¯¹æŠ—åšå¼ˆè§†è§’")[
      *æ”»å‡»è€…*ï¼ˆMembership Inferenceï¼‰ï¼š
      - $H_0$ï¼šæ•°æ®ç‚¹ $x$ ä¸åœ¨training set $D$ ä¸­
      - $H_1$ï¼šæ•°æ®ç‚¹ $x$ åœ¨training set $D$ ä¸­
      - ç›®æ ‡ï¼šä» $M(D)$ åŒºåˆ†ä¸¤ç§æƒ…å†µ

      *é˜²å¾¡è€…*ï¼ˆDPï¼‰ï¼š
      - ä½¿ $P[M(D) in S] approx P[M(D') in S]$
      - æ”»å‡»è€…çš„æ£€éªŒåŠŸæ•ˆ $approx$ éšæœºçŒœæµ‹
    ]
  ],
  [
    ```
    Hypothesis Testing è§†è§’ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Hâ‚€: x âˆ‰ Train (Out)     â”‚
    â”‚ Hâ‚: x âˆˆ Train (In)      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ æ”»å‡»è€…è§‚å¯Ÿ: M(D) æˆ– M(D')â”‚
    â”‚ åšå‡ºåˆ¤æ–­ â†’ Win if bÌ‚ = bâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    DPç›®æ ‡ï¼šä½¿åˆ¤æ–­â‰ˆéšæœºçŒœæµ‹
    ```
  ],
)

=== $epsilon$-DP é»„é‡‘å…¬å¼
#grid(
  columns: (1fr,1fr),
  [#theorem(title: "$epsilon$-Differential Privacy")[
  $ forall S, forall (D, D') "neighbors": quad P[M(D) in S] lt.eq e^epsilon dot P[M(D') in S] $

  å½“ $epsilon$ å¾ˆå°æ—¶ï¼š$e^epsilon approx 1 + epsilon$

  *åŒè¾¹ç•Œ*ï¼ˆåˆ©ç”¨é‚»å±…å¯¹ç§°æ€§ï¼‰ï¼š
  $ (1 - epsilon) P[M(D') in S] lt.eq P[M(D) in S] lt.eq (1 + epsilon) P[M(D') in S] $
]
  #tip[
  å®è·µä¸­ $epsilon = 5$ æˆ– $epsilon = 8$ å¾ˆå¸¸è§ï¼Œæ­¤æ—¶ $e^8 approx 2981$ï¼Œçº¿æ€§è¿‘ä¼¼*å®Œå…¨å¤±æ•ˆ*ï¼
]
],
  [=== $(epsilon, delta)$-DP æ”¾æ¾

#definition(title: "$(epsilon, delta)$-DP")[
  $ P[M(D) in S] lt.eq e^epsilon dot P[M(D') in S] + delta $

  *$delta$ çš„å«ä¹‰*ï¼šä¸æ˜¯"å…è®¸æ³„éœ²çš„prob"ï¼Œè€Œæ˜¯åˆ†å¸ƒå°¾éƒ¨çš„è´¨é‡ç•Œã€‚

  é€šå¸¸è¦æ±‚ $delta lt.double 1/n$ï¼ˆ$n$ æ˜¯datasetå¤§å°ï¼‰ã€‚
]],
)


=== é‚»å±…å…³ç³»çš„ä¸‰ç§å®šä¹‰

#figure(
  table(
    columns: 4,
    align: left,
    [*é‚»å±…å®šä¹‰*], [*åœºæ™¯*], [*æ•æ„Ÿåº¦*], [*å¯¹åº”å™ªå£°*],
    [$norm(D - D')_0 lt.eq 1$], [æ·»åŠ /åˆ é™¤ä¸€æ¡è®°å½•], [$Delta_1 f$], [Laplace],
    [$norm(D - D')_1 lt.eq 1$], [ä¿®æ”¹ä¸€ä¸ªç‰¹å¾], [$Delta_1 f$], [Laplace],
    [$norm(D - D')_2 lt.eq 1$], [è¿ç»­æ‰°åŠ¨ï¼ˆgradientï¼‰], [$Delta_2 f$], [Gaussian],
  ),
)

== ä¸¤å¤§åŸºç¡€æœºåˆ¶

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === Laplace æœºåˆ¶

    $ M(D) = f(D) + "Lap"(frac(Delta_1 f, epsilon)) $

    Laplace åˆ†å¸ƒï¼š$p(x) = frac(1, 2b) e^(-|x|/b)$

    å…¶ä¸­ $b = frac(Delta_1 f, epsilon)$

    *è¯æ˜å…³é”®æ­¥éª¤*ï¼š
    $ frac(p(M(D) = z), p(M(D') = z)) lt.eq e^epsilon $

    ä½¿ç”¨åä¸‰è§’ä¸ç­‰å¼ $|a| - |b| lt.eq |a - b|$
  ],
  [
    === Gaussian æœºåˆ¶

    $ M(D) = f(D) + cal(N)(0, sigma^2 I) $

    å…¶ä¸­ $sigma gt.eq frac(Delta_2 f dot sqrt(2 ln(1.25/delta)), epsilon)$

    *å¯¹æ¯”*ï¼š
    - Laplaceï¼šé‡å°¾ï¼ˆå¯èƒ½å¤§å™ªå£°ï¼‰
    - Gaussianï¼šè½»å°¾ï¼ˆå™ªå£°æ›´é›†ä¸­ï¼‰
    - Laplace é€‚åˆç¦»æ•£æŸ¥è¯¢
    - Gaussian é€‚åˆè¿ç»­gradientç©ºé—´
  ],
)

== DP ä¸‰å¤§é»„é‡‘æ€§è´¨

#grid(
  columns: (1fr, 1fr, 1fr),
  gutter: 0.8em,
  [
    === Post-Processing

    $ M "is" (epsilon, delta)"â€“DP" $
    $ arrow.r.double forall g: g compose M "is" (epsilon, delta)"â€“DP" $

    *ç›´è§‰*ï¼šå™ªå£°ä¸€æ—¦åŠ å…¥ï¼Œåç»­è®¡ç®—æ— æ³•"æçº¯"ã€‚
  ],
  [
    === Composition

    $ M_1 "is" (epsilon_1, delta_1)"â€“DP" $
    $ M_2 "is" (epsilon_2, delta_2)"â€“DP" $
    $ arrow.r.double (M_1, M_2) "is" (epsilon_1 + epsilon_2, delta_1 + delta_2)"â€“DP" $

    æ¯æ¬¡æŸ¥è¯¢éƒ½*æ¶ˆè€—*éšç§é¢„ç®—ï¼
  ],
  [
    === Subsampling

    å¯¹éšæœºå­é›† $Q = L/N$ åº”ç”¨ $(epsilon, delta)$-DPï¼š
    $ arrow.r.double (Q epsilon, Q delta)"â€“DP" $

    *ç›´è§‰*ï¼šä¸çŸ¥æ˜¯å¦è¢«é‡‡æ · â†’ éšç§å¢å¼ºã€‚
  ],
)

== DPSGD ç®—æ³•

#algorithm(title: "Differentially Private SGD")[
  ```python
  def DPSGD(data, model, C, Ïƒ, epochs):
    for epoch in range(epochs):
      for batch in sample_minibatch(data, L):
        gradients = []
        for (x, y) in batch:
          g = compute_gradient(model, x, y)
          # Step 1: gradientè£å‰ªï¼ˆæ§åˆ¶æ•æ„Ÿåº¦ï¼‰
          g_clip = g * min(1, C / ||g||â‚‚)
          gradients.append(g_clip)

        # Step 2: èšåˆ + æ·»åŠ å™ªå£°
        g_avg = mean(gradients)
        g_noisy = g_avg + N(0, ÏƒÂ²CÂ²/LÂ² Â· I)

        model = model - Î· * g_noisy
    return model
  ```
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === ä¸ºä»€ä¹ˆéœ€è¦gradientè£å‰ªï¼Ÿ

    $ Delta_2 g = max_(D tilde D') norm(g(D) - g(D'))_2 $

    *é—®é¢˜*ï¼šè‹¥å­˜åœ¨ outlier ä½¿ $norm(g)_2 arrow infinity$ï¼Œæ•æ„Ÿåº¦æ— ç•Œï¼

    *è§£å†³*ï¼šå¼ºåˆ¶ $norm(g)_2 lt.eq C$ï¼Œåˆ™æ•æ„Ÿåº¦ $Delta_2 lt.eq C$ã€‚
  ],
  [
    === éšç§é¢„ç®—ç´¯ç§¯

    å¯¹äº $T$ æ­¥è®­ç»ƒï¼Œæ¯æ­¥é‡‡æ ·æ¯”ä¾‹ $Q = L/N$ï¼š

    *æœ´ç´  Composition*ï¼š$(Q T epsilon, Q T delta)$-DP

    *é—®é¢˜*ï¼š$T$ å¯èƒ½æ˜¯ $10^6$ï¼Œé¢„ç®—*çˆ†ç‚¸*ï¼

    *æ”¹è¿›*ï¼šStrong Composition $epsilon_("total") = O(sqrt(T) dot epsilon)$
  ],
)

=== Privacy-Utility Trade-off

#figure(
  table(
    columns: 4,
    align: left,
    [*å‚æ•°*], [*â†‘ å¢å¤§*], [*å¯¹éšç§å½±å“*], [*å¯¹æ•ˆç”¨å½±å“*],
    [$epsilon$], [éšç§å˜å¼±], [â†“], [â†‘ï¼ˆå™ªå£°å‡å°‘ï¼‰],
    [$C$ï¼ˆè£å‰ªé˜ˆå€¼ï¼‰], [æ•æ„Ÿåº¦å¢å¤§], [â†“], [â†‘ï¼ˆä¿ç•™æ›´å¤šgradientä¿¡æ¯ï¼‰],
    [$sigma$ï¼ˆå™ªå£°ï¼‰], [åˆ†å¸ƒæ›´å®½], [â†‘], [â†“ï¼ˆä¿¡å·è¢«æ·¹æ²¡ï¼‰],
  ),
)

== PATEï¼šæ•™å¸ˆé›†æˆçš„ DP

#algorithm(title: "PATE (Private Aggregation of Teacher Ensembles)")[
  ```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ç§æœ‰æ•°æ® D = Dâ‚ âˆª Dâ‚‚ âˆª ... âˆª Dâ‚˜ (åˆ†æˆMä»½)                     â”‚
  â”‚                                                               â”‚
  â”‚ è®­ç»ƒMä¸ªæ•™å¸ˆ: Tâ‚, Tâ‚‚, ..., Tâ‚˜ (æ— DPï¼Œå„è‡ªç‹¬ç«‹)                  â”‚
  â”‚                                                               â”‚
  â”‚ å¯¹å…¬å¼€æœªæ ‡æ³¨æ•°æ® xï¼š                                           â”‚
  â”‚   - æ¯ä¸ªæ•™å¸ˆæŠ•ç¥¨: nâ±¼(x) = #{Táµ¢: Táµ¢(x) = j}                    â”‚
  â”‚   - èšåˆ + åŠ å™ª: Å· = argmax_j (nâ±¼(x) + Lap(2/Îµ))              â”‚
  â”‚                                    â†‘ å…³é”®ï¼šargmax ä¹‹å‰åŠ å™ªï¼   â”‚
  â”‚ ç”¨ (x, Å·) è®­ç»ƒå­¦ç”Ÿmodelï¼ˆå…¬å¼€å‘å¸ƒï¼‰                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ```
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === å™ªå£°æ·»åŠ ä½ç½®

    *é”™è¯¯*ï¼šargmax *ä¹‹å*åŠ å™ª
    - æ•æ„Ÿåº¦ = $|Y|$ï¼ˆæ ‡ç­¾ç©ºé—´å¤§å°ï¼‰

    *æ­£ç¡®*ï¼šargmax *ä¹‹å‰*åŠ å™ª
    - æ”¹å˜ä¸€ä¸ªsample â†’ ä¸€ä¸ªæ•™å¸ˆæŠ•ç¥¨å˜åŒ–
    - æŠ•ç¥¨å˜åŒ–ï¼š$+1$ å’Œ $-1$
    - *L1 æ•æ„Ÿåº¦ = 2*ï¼ˆä¸æ˜¯ $|Y|$ï¼‰
  ],
  [
    === éšç§é¢„ç®—

    æ¯æ¬¡æŸ¥è¯¢æ¶ˆè€— $epsilon_0$ï¼š
    $ T "æ¬¡æŸ¥è¯¢" arrow.r.double T epsilon_0 "-DP" $

    *å®è·µæ„ä¹‰*ï¼šå…¬å¼€datasetè§„æ¨¡å—é™äºéšç§é¢„ç®—ï¼

    *ä¼˜åŒ–*ï¼šä½¿ç”¨ Confident-GNMax ç­‰æ–¹æ³•å‡å°‘æ¯æ¬¡æŸ¥è¯¢çš„é¢„ç®—æ¶ˆè€—ã€‚
  ],
)

== Federated Learning + DP

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === FedSGD + DP

    *Client $k$*ï¼š
    1. è®¡ç®—gradientï¼š$g_k = nabla cal(L)(theta, D_k)$
    2. è£å‰ªï¼š$g_k arrow.l g_k dot min(1, C/norm(g_k)_2)$
    3. åŠ å™ªï¼š$g_k arrow.l g_k + cal(N)(0, sigma^2 I)$
    4. å‘é€ $g_k$ ç»™ Server

    *Server*ï¼š$theta arrow.l theta - eta dot frac(1, K) sum_k g_k$
  ],
  [
    === FedAvg + DP åŒºåˆ«

    #figure(
      table(
        columns: 2,
        align: left,
        [*FedSGD + DP*], [*FedAvg + DP*],
        [å‘é€å•æ­¥gradient], [å‘é€å¤šæ­¥æƒé‡å·®],
        [å¯¹ $g_k$ åŠ å™ª], [å¯¹ $Delta theta_k$ åŠ å™ª],
        [ç›´æ¥åº”ç”¨ Gaussian], [éœ€è€ƒè™‘å¤šæ­¥ä¾èµ–],
      ),
    )
  ],
)

== Model Stealing Attack

#definition(title: "modelçªƒå–æ”»å‡»")[
  *ç›®æ ‡*ï¼šé€šè¿‡ API æŸ¥è¯¢ï¼Œå¤åˆ¶ç›®æ ‡modelçš„åŠŸèƒ½

  *å½¢å¼åŒ–*ï¼šç»™å®šåªèƒ½ query çš„ $f_("target")$ï¼Œè®­ç»ƒ $f_("copy")$ ä½¿ï¼š
  $ forall x: f_("copy")(x) approx f_("target")(x) $
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === æ–¹æ³•

    1. *Query-based*ï¼š
      - ç”Ÿæˆå¤§é‡ $(x, f_("target")(x))$ å¯¹
      - ç”¨çŸ¥è¯†è’¸é¦è®­ç»ƒ $f_("copy")$

    2. *Side-channel*ï¼š
      - åˆ©ç”¨ API è¿”å›çš„ logits/confidence
      - æ¨æ–­æ›´å¤šmodelä¿¡æ¯
  ],
  [
    === é˜²å¾¡

    - *Rate Limiting*ï¼šé™åˆ¶æŸ¥è¯¢æ¬¡æ•°
    - *Output Perturbation*ï¼šæ·»åŠ å™ªå£°åˆ°è¾“å‡º
    - *Query Auditing*ï¼šæ£€æµ‹å¯ç–‘æŸ¥è¯¢æ¨¡å¼
    - *Watermarking*ï¼šåœ¨modelä¸­åµŒå…¥æ°´å°ï¼Œè¯æ˜æ‰€æœ‰æƒ
  ],
)

== Model Inversion Attack

#definition(title: "modelåæ¼”æ”»å‡»")[
  *ç›®æ ‡*ï¼šä»modelè¾“å‡º*é‡å»º*è®­ç»ƒæ•°æ®çš„*ä»£è¡¨æ€§*sample

  ä¸ Gradient Inversion åŒºåˆ«ï¼š
  - Gradient Inversionï¼šç²¾ç¡®é‡å»º*å…·ä½“*sampleï¼ˆFL åœºæ™¯ï¼‰
  - Model Inversionï¼šé‡å»º*ç±»åˆ«çš„å…¸å‹*sampleï¼ˆé»‘ç›’åœºæ™¯ï¼‰
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === æ”»å‡»å…¬å¼

    $ x^* = arg max_x P(y_("target") | x) $

    æˆ–ä½¿ç”¨ GAN ç”Ÿæˆï¼š
    $ z^* = arg max_z f_("target")(G(z))_y $

    å†è®¡ç®— $x^* = G(z^*)$
  ],
  [
    === å¯è§†åŒ–

    ```
    Class "Person A" (Label 7)
           â†“
    Model Inversion Optimization
           â†“
    ç”Ÿæˆä¸€å¼ çœ‹èµ·æ¥åƒ "Person A" çš„è„¸
    ï¼ˆä¸æ˜¯training setä¸­çš„å…·ä½“ç…§ç‰‡ï¼‰
    ```
  ],
)

#tip[
  Model Inversion ç”Ÿæˆçš„æ˜¯*ç±»åˆ«çš„å¹³å‡ç‰¹å¾*ï¼Œä¸æ˜¯å…·ä½“ä¸ªäººçš„ç²¾ç¡®ç…§ç‰‡ã€‚ä½†å¯¹äºæ•æ„Ÿç±»åˆ«ï¼ˆå¦‚äººè„¸ï¼‰ï¼Œè¿™ä»ç„¶æ˜¯ä¸¥é‡çš„éšç§æ³„éœ²ã€‚
]

== Membership Inference Attack (MIA)

#definition(title: "MIA é—®é¢˜è®¾å®š")[
  ç»™å®šmodel $M$ å’Œsample $x$ï¼Œåˆ¤æ–­ $x in D_("train")$ï¼Ÿ

  *å½¢å¼åŒ–ä¸ºå‡è®¾æ£€éªŒ*ï¼š
  - $H_0$ï¼š$x in.not D_("train")$ï¼ˆOutï¼‰
  - $H_1$ï¼š$x in D_("train")$ï¼ˆInï¼‰
  - å†³ç­–è§„åˆ™ï¼š$"Score"(x) > tau arrow$ Reject $H_0$ï¼ˆåˆ¤å®šä¸º Inï¼‰
]

=== Shadow Model æ–¹æ³•

#algorithm(title: "Shadow Model Attack")[
  ```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ PHASE 1: Shadow Model Training                            â”‚
  â”‚   è®­ç»ƒ K ä¸ª Shadow Models: Mâ‚, Mâ‚‚, ..., Mâ‚–               â”‚
  â”‚   æ¯ä¸ªç”¨ä¸åŒçš„training set                                       â”‚
  â”‚                                                           â”‚
  â”‚ PHASE 2: Attack Classifier Training                       â”‚
  â”‚   å¯¹æ¯ä¸ª Shadow Model i:                                  â”‚
  â”‚     - x âˆˆ Dáµ¢ (IN):  (Modeláµ¢(x), IN)                      â”‚
  â”‚     - x âˆ‰ Dáµ¢ (OUT): (Modeláµ¢(x), OUT)                     â”‚
  â”‚   â†’ è®­ç»ƒ Attack Classifier A                              â”‚
  â”‚                                                           â”‚
  â”‚ PHASE 3: Attack Target Model                              â”‚
  â”‚   Target Model M, query point x                          â”‚
  â”‚   â†’ bÌ‚ = A(M(x))                                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ```
]

#tip[
  å¯¹ LLM *å®Œå…¨ä¸é€‚ç”¨*â€”â€”è°èƒ½è®­ç»ƒ 64 ä¸ª GPT-4ï¼Ÿ
]

=== Score-Based æ–¹æ³•ï¼ˆç°ä»£æ–¹æ³•ï¼‰

#figure(
  table(
    columns: 3,
    align: left,
    [*æ–¹æ³•*], [*Score å…¬å¼*], [*ç›´è§‰*],
    [Loss], [$-log P_M (x)$], [è®­ç»ƒæ•°æ® loss æ›´ä½],
    [LiRA], [$log frac(P(ell | x in S), P(ell | x in.not S))$], [è´å¶æ–¯ä¼¼ç„¶æ¯”],
    [Min-K% Prob], [$frac(1, K) sum_(i in "bottom-K") log P(x_i)$], [ä½prob token å¯¹ member ä¹Ÿè¾ƒé«˜],
  ),
)

=== LiRA è¯¦è§£ï¼ˆLikelihood Ratio Attackï¼‰

#definition(title: "LiRA æ ¸å¿ƒæ€æƒ³")[
  *è´å¶æ–¯è§†è§’*ï¼šä¸åªçœ‹modelå¯¹ $x$ çš„ lossï¼Œè€Œæ˜¯æ¯”è¾ƒ"modelåœ¨ $x$ ä¸Šçš„è¡Œä¸º"ä¸"éšæœºmodelåœ¨ $x$ ä¸Šçš„è¡Œä¸º"ã€‚

  $ "LiRA Score" = log frac(P(ell(x) | x in D), P(ell(x) | x in.not D)) $
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === å®ç°æ­¥éª¤

    1. *è®­ç»ƒ MANY Shadow Models*ï¼š
      - ä¸€åŠåŒ…å« $x$ï¼ˆIN é›†åˆï¼‰
      - ä¸€åŠä¸åŒ…å« $x$ï¼ˆOUT é›†åˆï¼‰

    2. *ä¼°è®¡åˆ†å¸ƒ*ï¼š
      - $P(ell | "IN") = cal(N)(mu_("IN"), sigma_("IN")^2)$
      - $P(ell | "OUT") = cal(N)(mu_("OUT"), sigma_("OUT")^2)$

    3. *è®¡ç®— Log-Likelihood Ratio*ï¼š
      $ "Score" = frac((ell - mu_("OUT"))^2, 2 sigma_("OUT")^2) - frac((ell - mu_("IN"))^2, 2 sigma_("IN")^2) $
  ],
  [
    === ä¸ºä»€ä¹ˆæ¯” Loss æ›´å¥½ï¼Ÿ

    #note[
      *Loss-based*ï¼šåªçœ‹ç»å¯¹å€¼
      - é—®é¢˜ï¼šç®€å•sample loss æœ¬å°±ä½

      *LiRA*ï¼šçœ‹ç›¸å¯¹å˜åŒ–
      - è§£å†³ï¼šæ§åˆ¶äº†sampleéš¾åº¦å·®å¼‚
    ]

    ```
    Loss-based:
      ç®€å•sample: loss=0.1 (member)
      å›°éš¾sample: loss=0.5 (member)
      â†’ å®¹æ˜“è¯¯åˆ¤å›°éš¾sampleä¸º non-member

    LiRA:
      æ¯”è¾ƒ IN vs OUT çš„ loss åˆ†å¸ƒ
      â†’ å¯¹sampleéš¾åº¦ robust
    ```
  ],
)

#tip[
  *LiRA çš„å±€é™*ï¼š
  - éœ€è¦è®­ç»ƒå¤§é‡ Shadow Modelsï¼ˆ>256ï¼‰
  - å¯¹ LLM ä¸å¯è¡Œ
  - æœ€æ–°è¶‹åŠ¿ï¼šå•model LiRA å˜ç§ï¼ˆç”¨æ•°æ®å¢å¼ºä»£æ›¿å¤šmodelï¼‰
]

=== MIA å®é™…è¡¨ç°

#tip[
  *AUC $approx$ 0.5~0.7*ï¼šæ¥è¿‘éšæœºçŒœæµ‹ï¼

  *Low FPR åŒºåŸŸæ‰é‡è¦*ï¼šå½“ FPR = 0.01 æ—¶ï¼ŒTPR å¯èƒ½åªæœ‰ 2%ã€‚

  è¿™æ„å‘³ç€è¯¯æŠ¥ç‡æé«˜â€”â€”å®è·µä¸­ MIA *å‡ ä¹ä¸ work*ã€‚
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    ```
           TPR
            â†‘
       1.0 â”€â”¤         Â·Â·Â·Â·Â·Â·Â·Â·Â·
            â”‚       Â·
            â”‚     Â·    â† é«˜FPRåŒºåŸŸ
            â”‚   Â·         æ„ä¹‰ä¸å¤§
            â”‚ Â·
       0.02 â”€â”¼Â·â”€â”€â”€â”€â”€â”€â”€â”€  â† ä½FPRåŒºåŸŸ
            â”‚              TPRåªæœ‰2%
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ FPR
            0    0.01  0.1  1.0
    ```
  ],
  [
    *ä¸ºä»€ä¹ˆä½ FPR é‡è¦ï¼Ÿ*
    - training set $|S| lt.double |D backslash S|$
    - å³ä½¿å° FPR ä¹Ÿæ„å‘³ç€å¤§é‡ false positives
    - å®é™…éƒ¨ç½²ä¸­æ— æ³•æ‰¿å—
  ],
)

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    == Dataset Inference

    #definition(title: "ä»å•ç‚¹åˆ°dataset")[
      *åŠ¨æœº*ï¼š
      - å•ç‚¹ MIA å¤ªéš¾ã€å¤ª expensive
      - æ•°æ®æ‹¥æœ‰è€…é€šå¸¸æœ‰*æ•´ä¸ªdataset*
      - å¼±ä¿¡å·èšåˆå â†’ å¼ºä¿¡å·
    ]

    *T æ£€éªŒ*ï¼š
    $
      t = frac(overline(x)_("pub") - overline(x)_("val"), sqrt(frac(s_("pub")^2, n_("pub")) + frac(s_("val")^2, n_("val"))))
    $

    è‹¥ p-value $< alpha$ï¼Œåˆ™ reject $H_0$ â†’ datasetè¢«ä½¿ç”¨ã€‚
  ],
  [
    == Memorization

    #definition(title: "K-Extractable")[
      å­—ç¬¦ä¸² $S$ æ˜¯ K-extractableï¼Œè‹¥å­˜åœ¨ prefix $P$ï¼š
      $ P || S in D_("train") and M(P) = S "ï¼ˆgreedy decodingï¼‰" $
    ]

    *å½±å“å› ç´ *ï¼š

    #figure(
      table(
        columns: 3,
        align: left,
        [*å› ç´ *], [*å…³ç³»*], [*åŸå› *],
        [Model Size], [æ­£ç›¸å…³], [æ›´å¤§å®¹é‡ â†’ æ›´èƒ½"è®°ä½"],
        [Prefix Length], [æ­£ç›¸å…³], [æ›´å¤š context â†’ æ›´çª„çš„ continuation åˆ†å¸ƒ],
        [Repetition], [æ­£ç›¸å…³], [gradientæ›´æ–°è¶Šå¤š â†’ è®°å¾—è¶Šç‰¢],
        [Sequence Length], [è´Ÿç›¸å…³], [ç´¯ç§¯é”™è¯¯],
      ),
    )
  ],
)



== DP ä¸ RS çš„å¯¹å¶æ€§

#theorem(title: "åŒä¸€æšç¡¬å¸çš„ä¸¤é¢")[
  #figure(
    table(
      columns: 3,
      align: left,
      [], [*Differential Privacy*], [*Randomized Smoothing*],
      [ç›®æ ‡], [ä½¿åˆ†å¸ƒ*ç›¸ä¼¼*ï¼ˆä¸å¯åŒºåˆ†ï¼‰], [ä½¿åˆ†å¸ƒ*ä¸åŒ*ï¼ˆå¯åŒºåˆ†ï¼‰],
      [æ•°å­¦], [$P[M(D)] lt.eq e^epsilon P[M(D')]$], [$P[f(x+eta)=c] > e^(2 epsilon) P[f(x+eta)=c']$],
      [å™ªå£°ä½œç”¨], [æ··æ·†çœŸå®æ•°æ®], [å¹³æ»‘å†³ç­–è¾¹ç•Œ],
    ),
  )

  *ç»Ÿä¸€è§†è§’*ï¼š
  - DPï¼šå¸Œæœ›å‡è®¾æ£€éªŒ Power *ä½*
  - RSï¼šå¸Œæœ›åˆ†ç±»ç½®ä¿¡åº¦ *é«˜*

  ä¸¤è€…ä½¿ç”¨*ç›¸åŒæ•°å­¦å·¥å…·*ï¼ˆæŒ‡æ•°ç•Œã€å™ªå£°åˆ†å¸ƒï¼‰ï¼Œä½†*ä¼˜åŒ–æ–¹å‘ç›¸å*ã€‚
]

== Gradient Inversion Attack

#definition(title: "gradientåæ¼”æ”»å‡»")[
  *æ ¸å¿ƒå‡è®¾*ï¼šgradient $nabla theta$ å¿…é¡»åŒ…å«æ•°æ®ä¿¡æ¯æ‰èƒ½ä¼˜åŒ– â†’ å¯åæ¨æ•°æ®

  *æ”»å‡»ç›®æ ‡*#footnote[é€‚ç”¨äº FL åœºæ™¯ï¼Œæ”»å‡»è€…ï¼ˆæ¶æ„ Serverï¼‰å¯è§‚å¯Ÿå®¢æˆ·ç«¯ä¸Šä¼ çš„gradient]ï¼š
  $ x^* = arg min_x norm(nabla theta cal(L)(x, y) - nabla_("obs"))^2 + lambda R(x) $

  å…¶ä¸­ $R(x)$ æ˜¯æ¨¡æ€ç‰¹å®šçš„ Priorã€‚
]

#grid(
  columns: (1fr, 1fr, 1fr),
  gutter: 0.8em,
  [
    === Image Prior

    *Total Variation*ï¼š
    $ R_("TV")(x) = sum_(i,j) |x_(i+1,j) - x_(i,j)| $

    é¼“åŠ±å›¾åƒå¹³æ»‘ã€‚
  ],
  [
    === Text Prior

    *Perplexity + Reorder*ï¼š

    åˆ©ç”¨LMçš„probå’Œç¦»æ•£ä¼˜åŒ–ã€‚
  ],
  [
    === Tabular Prior

    *Entropy-based*ï¼š

    åˆ©ç”¨ç±»åˆ«åˆ†å¸ƒå‡è®¾ç­›é€‰å€™é€‰ã€‚
  ],
)

=== FedSGD vs FedAvg æ”»å‡»éš¾åº¦

#figure(
  table(
    columns: 3,
    align: left,
    [], [*FedSGD*], [*FedAvg*],
    [å‘é€å†…å®¹], [å•æ­¥gradient $nabla theta$], [å¤šæ­¥æ›´æ–° $Delta theta$],
    [æ”»å‡»éš¾åº¦], [è¾ƒæ˜“ï¼ˆç›´æ¥gradientåŒ¹é…ï¼‰], [è¾ƒéš¾ï¼ˆéœ€åæ¼”ä¼˜åŒ–è½¨è¿¹ï¼‰],
    [æ”»å‡»å…¬å¼], [$min norm(nabla - nabla^*)$], [$min sum_(e,b) norm(nabla_(e,b) - nabla_(e,b)^*)$],
  ),
)

#note[
  FedAvg æ”»å‡»éœ€è¦åˆ©ç”¨*è·¨ epoch æ•°æ®ä¸€è‡´æ€§*å…ˆéªŒï¼š$X_(e_1, b) approx X_(e_2, b)$ã€‚
]

== Attribute Inference

#definition(title: "è¶…è¶Š Membership çš„éšç§æ”»å‡»")[
  ç»™å®šmodel $M$ å’Œ*éƒ¨åˆ†å…¬å¼€å±æ€§* $x_("pub")$ï¼Œæ¨æ–­*æ•æ„Ÿå±æ€§* $x_("sens")$ï¼š
  $ hat(x)_("sens") = arg max_(x_("sens")) P(M | x_("pub"), x_("sens")) $

  *å…³é”®åŒºåˆ«*ï¼š*ä¸éœ€è¦* $x$ åœ¨training setä¸­ï¼åªéœ€modelå­¦åˆ°äº†å±æ€§ç›¸å…³æ€§ã€‚
]

#example(title: "ä»æ–‡æœ¬æ¨æ–­åœ°ç†ä½ç½®")[
  è¾“å…¥ï¼š"left shark thing is hilarious... seen it after final exams"

  LLM æ¨ç†ï¼šGlendale, AZï¼ˆ2015 Super Bowl ä¸¾åŠåœ°ï¼‰

  *å‡†ç¡®ç‡*ï¼š85% Top-1
]

== éšç§æ”»å‡»å±‚æ¬¡å…³ç³»

#grid(
  columns: (1fr, 1.2fr),
  gutter: 1em,
  [
    ```
        ä¿¡æ¯æ³„éœ²ä¸¥é‡ç¨‹åº¦ â†‘
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Attribute Inference â”‚ â† æœ€å¼º
    â”‚ (æ¨æ–­æ•æ„Ÿå±æ€§)       â”‚   ä¸éœ€è¦membership
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Data Extraction     â”‚
    â”‚ (ç²¾ç¡®é‡æ„æ•°æ®)       â”‚ â† Memorization
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Membership Inferenceâ”‚ â† åŸºç¡€
    â”‚ (åˆ¤æ–­æ˜¯å¦åœ¨training set)   â”‚   äºŒåˆ†ç±»é—®é¢˜
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Dataset Inference   â”‚ â† å¯¹æŠ—å•ç‚¹MIA
    â”‚ (datasetçº§åˆ«èšåˆ)     â”‚   ç»Ÿè®¡æ£€éªŒ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
  ],
  [
    #theorem(title: "å±‚æ¬¡å…³ç³»")[
      - *Memorization â†’ Membership*ï¼šèƒ½é€å­—é‡å¤ â†’ ä¸€å®šåœ¨training setä¸­
      - *Membership â†› Memorization*ï¼šåœ¨training setä¸­ $eq.not$ ä¼šè¢« memorize
      - *Attribute Inference ç‹¬ç«‹äº Membership*ï¼šå³ä½¿æ•°æ®ä¸åœ¨training setï¼Œä¹Ÿå¯èƒ½é€šè¿‡äº¤äº’æ³„éœ²å±æ€§
    ]
  ],
)

== Agentic AI å®‰å…¨

#definition(title: "Indirect Prompt Injection (IPI)")[
  *æ”»å‡»é“¾*ï¼š
  (æ ¸å¿ƒé—®é¢˜ï¼šmodelæ— æ³•åŒºåˆ†"ç”¨æˆ·æŒ‡ä»¤"vs"å·¥å…·è¾“å‡ºä¸­çš„æŒ‡ä»¤")
  ```
  Attacker â”€â”€â–º Environment â”€â”€â–º Agent â”€â”€â–º Sensitive Action
  (å‘é€é‚®ä»¶)    (æ”¶ä»¶ç®±)      (Cursor/GPT)  (è¯»å–ç§æœ‰ä»“åº“)
  ```

  *æ”»å‡»å‘é‡*ï¼šé€šè¿‡ä¸å¯ä¿¡ç¯å¢ƒï¼ˆé‚®ä»¶/ç½‘é¡µï¼‰æ³¨å…¥æŒ‡ä»¤ã€‚
]

#grid(
  columns: (1fr, 1fr, 1fr),
  gutter: 0.8em,
  [
    === Instruction Hierarchy

    è®­ç»ƒmodelåŒºåˆ† System/User/Tool æƒé™ç­‰çº§

    *å±€é™*ï¼šä½æƒé™å†…å®¹ä»å¯é€šè¿‡è¯­ä¹‰å½±å“é«˜æƒé™å†³ç­–
  ],
  [
    === Command Sense

    æ£€æµ‹å¹¶ç§»é™¤"AI æŒ‡ä»¤è¯­æ°”"çš„æ–‡æœ¬

    *å±€é™*ï¼šæ— æ³•æ•æ‰éæŒ‡ä»¤å¼æ”»å‡»
  ],
  [
    === Dual-LLM Pattern

    Planner ä¸çœ‹å·¥å…·è¾“å‡ºï¼ŒExecutor ä¸çœ‹ç”¨æˆ·æŒ‡ä»¤

    *å±€é™*ï¼šæ— æ³•å¤„ç†åŠ¨æ€å†³ç­–
  ],
)

#tip[
  *æ ¸å¿ƒå¼ åŠ›*ï¼š$"Security" prop 1/"Capability"$;
  Planner ä¸çœ‹å·¥å…·è¾“å‡º â†’ å®‰å…¨ä½†æ— æ³•åšåŠ¨æ€ä»»åŠ¡ï¼ˆå¦‚"æ ¹æ®é‚®ä»¶å†…å®¹å†³å®šä¸‹ä¸€æ­¥"ï¼‰
]

== æ•æ„Ÿåº¦çš„ç»Ÿä¸€è§†è§’

#theorem(title: "æ•æ„Ÿåº¦åŒæ—¶é‡åŒ–æ”»å‡»èƒ½åŠ›å’Œé˜²å¾¡ä»£ä»·")[
  *åœ¨æ”»å‡»ä¸­*ï¼šæ•æ„Ÿåº¦é«˜ â†’ gradientä¿¡æ¯é‡å¤§ â†’ æ˜“åæ¨æ•°æ®

  *åœ¨é˜²å¾¡ä¸­*ï¼šæ•æ„Ÿåº¦é«˜ â†’ éœ€è¦æ›´å¤šå™ªå£° â†’ æ•ˆç”¨losså¤§

  $ sigma = frac(Delta_2 f dot sqrt(2 ln(1.25/delta)), epsilon) $

  *è£å‰ªæ˜¯äººä¸ºæ§åˆ¶æ•æ„Ÿåº¦çš„æ‰‹æ®µ*ï¼Œè¿™è§£é‡Šäº† DPSGD çš„gradientè£å‰ªå’Œ PATE çš„ argmax å‰åŠ å™ªè®¾è®¡ã€‚
]

#figure(
  table(
    columns: 4,
    align: left,
    [*æ•æ„Ÿåº¦ç±»å‹*], [*å®šä¹‰*], [*ç”¨é€”*], [*æœºåˆ¶*],
    [$Delta_0$ (Hamming)], [æ·»åŠ /åˆ é™¤ä¸€æ¡è®°å½•], [Membership], [â€”],
    [$Delta_1$ (L1)], [$max norm(f(D) - f(D'))_1$], [è®¡æ•°æŸ¥è¯¢], [Laplace],
    [$Delta_2$ (L2)], [$max norm(f(D) - f(D'))_2$], [gradientç©ºé—´], [Gaussian],
  ),
)

== MIA Score å‡½æ•°æ€»è§ˆ

#figure(
  table(
    columns: 4,
    align: left,
    [*æ–¹æ³•*], [*Signal(x)*], [*Baseline(x)*], [*ç›´è§‰*],
    [Loss-based], [$-log p_theta (y|x)$], [å¸¸æ•°é˜ˆå€¼], [è®­ç»ƒæ•°æ® loss æ›´ä½],
    [Likelihood-Ratio], [$-log p_theta (y|x)$], [$-log p_("ref")(y|x)$], [ç›¸å¯¹äºåŸºå‡†model],
    [Gradient Norm], [$norm(nabla_theta cal(L)(x))$], [ç»éªŒåˆ†å¸ƒ], [è®­ç»ƒæ•°æ®gradientæ›´å°],
    [Calibration], [Conf(x) - Acc(x)], [0], [è¿‡æ‹Ÿåˆsampleè¿‡åº¦è‡ªä¿¡],
    [Min-K Prob], [å¹³å‡ K ä¸ªæœ€ä½ token prob], [ç»å¯¹é˜ˆå€¼], [ç½•è§ token ä¹Ÿæœ‰é«˜prob],
  ),
)

#note[
  *ç»Ÿä¸€æ´å¯Ÿ*ï¼šæ‰€æœ‰æ–¹æ³•éƒ½åœ¨æ‰¾"modelå¯¹è®­ç»ƒæ•°æ®çš„å¼‚å¸¸è‡ªä¿¡"ã€‚
]

== è€ƒè¯•æ¨¡å¼è¯†åˆ«

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === åœºæ™¯ â†’ å¨èƒç±»å‹æ˜ å°„

    #figure(
      table(
        columns: 2,
        align: left,
        [*åœºæ™¯æè¿°*], [*å¨èƒç±»å‹*],
        [æ”»å‡»è€…å¯ä»¥æŸ¥è¯¢model], [MIA, Model Inversion],
        [æ”»å‡»è€…èƒ½çœ‹åˆ°gradient], [Gradient Inversion (FL)],
        [modelè¾“å‡ºé€å­—é‡å¤], [Memorization],
        [æ¨æ–­ç”¨æˆ·æ•æ„Ÿå±æ€§], [Attribute Inference],
        [åˆ¤æ–­æ•°æ®æ˜¯å¦è¢«ä½¿ç”¨], [Dataset Inference],
      ),
    )
  ],
  [
    === å¯¹æ¯”/è¾¨æé¢˜è¦ç‚¹

    #figure(
      table(
        columns: 2,
        align: left,
        [*å¯¹æ¯”é¡¹*], [*åŒºåˆ†è¦ç‚¹*],
        [DP vs RS], [ç›®æ ‡ç›¸åï¼Œå·¥å…·ç›¸åŒ],
        [$epsilon$-DP vs $(epsilon, delta)$-DP], [ç›¸å¯¹ç•Œ vs ç›¸å¯¹+ç»å¯¹],
        [Laplace vs Gaussian], [$L_1$ vs $L_2$ æ•æ„Ÿåº¦],
        [MIA vs Dataset Inference], [å•ç‚¹å¼±ä¿¡å· vs èšåˆå¼ºä¿¡å·],
        [Memorization vs Inversion], [ç²¾ç¡®é€å­— vs ä»£è¡¨æ€§é‡æ„],
      ),
    )
  ],
)

== Privacy æ˜“é”™ç‚¹

*$delta$ çš„å«ä¹‰*ï¼šä¸æ˜¯"æ³„éœ²prob"ï¼Œè€Œæ˜¯å°¾éƒ¨è´¨é‡ç•Œ

*æ•æ„Ÿåº¦è®¡ç®—*ï¼šPATE åœ¨ argmax *ä¹‹å‰*åŠ å™ªï¼Œæ•æ„Ÿåº¦æ˜¯ 2 ä¸æ˜¯ $|Y|$

*éšç§é¢„ç®—ç´¯ç§¯*ï¼š$epsilon_("total") approx sqrt(T) dot epsilon$ï¼ˆAdvanced Compositionï¼‰ï¼Œéçº¿æ€§ç´¯ç§¯æ˜¯å®ç”¨åŒ–å…³é”®

*MIA å®è·µè¡¨ç°*ï¼šAUC $approx$ 0.5~0.7ï¼Œä½ FPR æ—¶ TPR æä½ï¼ˆ2% @ FPR=0.01%ï¼‰

*æ—¶é—´åç§»é™·é˜±*ï¼šç”¨æ—¶é—´åˆ‡åˆ†è¯„ä¼° MIA ä¼šæ··æ·†"æ—¶é—´"ä¸"membership"

*Gradient Inversion*ï¼šFedAvg æ¯” FedSGD éš¾æ”»ï¼Œéœ€å¤š epoch è€¦åˆä¼˜åŒ–

*Attribute Inference*ï¼š*ä¸éœ€è¦* membershipï¼åˆ©ç”¨å±æ€§ç›¸å…³æ€§

*Agentic AI*ï¼šSecurity $prop$ 1/Capabilityï¼Œå®Œå…¨éš”ç¦»ä¼šç‰ºç‰²åŠ¨æ€èƒ½åŠ›

= Part 8: Watermarking & Benchmarking <sec:watermark>

== LLM Watermarking æ ¸å¿ƒæ€æƒ³

#definition(title: "ä¸ºä»€ä¹ˆéœ€è¦ Watermarkï¼Ÿ")[
  *é—®é¢˜*ï¼šå¦‚ä½•è¯æ˜å†…å®¹æ˜¯ AI ç”Ÿæˆçš„ï¼Ÿï¼ˆAttribution Problemï¼‰

  #figure(
    table(
      columns: 2,
      align: left,
      [*æ–¹æ³•*], [*é—®é¢˜*],
      [Passive Detection (GPT-0)], [éšç€modelå˜å¼ºè¶Šæ¥è¶Šéš¾],
      [Visible Watermark (Sora logo)], [å®¹æ˜“è¢«ç§»é™¤],
      [Metadata], [æˆªå›¾å°±æ²¡äº†],
      [Fingerprinting (å“ˆå¸Œæ•°æ®åº“)], [éšç§é—®é¢˜ + æ•°æ®åº“çˆ†ç‚¸],
      [*Invisible Watermark* âœ“], [åµŒå…¥ç”Ÿæˆè¿‡ç¨‹ï¼Œäººç±»ä¸å¯å¯Ÿè§‰],
    ),
  )
]

== Red-Green Watermark (KGW)

#grid(
  columns: (1.2fr, 1fr),
  gutter: 1em,
  [
    #definition(title: "æ ¸å¿ƒæ€æƒ³")[
      å°†è¯è¡¨#footnote[Vocabularyï¼Œmodelå¯ç”Ÿæˆçš„æ‰€æœ‰ token é›†åˆ]ä¼ªéšæœºåˆ†ä¸º *Green* å’Œ *Red*ï¼Œåå‘é‡‡æ · Green tokensã€‚

      $ cal(V) = underbrace(cal(G), "Green List") union underbrace(cal(R), "Red List") $

      å…¶ä¸­ $|cal(G)| = gamma |cal(V)|$ï¼ˆé€šå¸¸ $gamma = 0.5$ï¼‰ã€‚
    ]
  ],
  [
    ```
    hash(å‰hä¸ªtoken) + secret_key
              â†“
       seed â†’ PRG â†’ åˆ’åˆ†è¯è¡¨
              â†“
      Î³|V| ä¸ªGreen, (1-Î³)|V| ä¸ªRed
    ```
  ],
)

=== Generate å‡½æ•°

#algorithm(title: "Red-Green æ°´å°ç”Ÿæˆ")[
  *Step 1*ï¼šLLM è®¡ç®— logits $ell$ (ä¸‹ä¸€ä¸ª token çš„probåˆ†å¸ƒ)

  *Step 2*ï¼šç”¨ hash(context) + secret\_key ç¡®å®š Green/Red åˆ’åˆ†

  *Step 3*ï¼šä¿®æ”¹ logitsï¼Œç»™ Green tokens åŠ  $delta$ï¼š
  $ ell'_i = cases(ell_i + delta & "if token" i in "Green", ell_i & "if token" i in "Red") $

  *Step 4*ï¼šSoftmax é‡‡æ ·ï¼š$P("token"_i) = frac(e^(ell'_i), sum_j e^(ell'_j))$
]

#note[
  å…³é”®å‚æ•°ï¼š
  - $gamma$ï¼šGreen tokens æ¯”ä¾‹ï¼ˆé€šå¸¸ 0.5ï¼‰
  - $delta$ï¼šåç½®å¼ºåº¦ï¼ˆè¶Šå¤§æ°´å°è¶Šå¼ºï¼Œä½†è´¨é‡lossè¶Šå¤§ï¼‰
  - $h$ï¼šcontext çª—å£å¤§å°ï¼ˆç”¨å¤šå°‘å‰ç½® token åš hashï¼‰
]

=== Detect å‡½æ•°

#theorem(title: "æ£€æµ‹æ— éœ€ LLMï¼Œåªéœ€ secret keyï¼")[
  *ç»Ÿè®¡æ£€éªŒ*å‡è®¾æ£€éªŒï¼š$H_0$ä¸ºæ— æ°´å°ï¼Œæ¯ä¸ªtokené¢œè‰²éšæœºï¼š
  $ H_0: "æ— æ°´å°" arrow.r.double S tilde "Binomial"(n, 0.5) $

  å…¶ä¸­ $S$ æ˜¯ Green token è®¡æ•°ã€‚

  *P-value*ï¼š$P(X gt.eq S | H_0) = sum_(k=S)^n binom(n, k) 0.5^n$

  *åˆ¤å®šè§„åˆ™*ï¼šè‹¥ p-value $< alpha$ åˆ™åˆ¤å®šæœ‰æ°´å°ã€‚
]

#tip[
  $alpha$ ç›´æ¥æ§åˆ¶ False Positive Rateï¼è®¾ $alpha = 10^(-6)$ æ„å‘³ç€æ¯ç™¾ä¸‡æ¬¡è¯¯åˆ¤ä¸€æ¬¡ã€‚
]

== ITS Watermarkï¼ˆDistortion-Freeï¼‰

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === Red-Green çš„é—®é¢˜

    Red-Green *ä¿®æ”¹äº†probåˆ†å¸ƒ*ï¼

    ä¾‹å¦‚ï¼šBarack æ˜¯ Greenï¼ŒObama æ˜¯ Red â†’ å¯èƒ½é‡‡æ ·ä¸å‡º "Barack Obama"

    === Distortion-Free æ ¸å¿ƒ

    åœ¨*æœŸæœ›æ„ä¹‰*ä¸Šä¸æ”¹å˜ LLM çš„è¾“å‡ºåˆ†å¸ƒã€‚
  ],
  [
    #algorithm(title: "ITS é‡‡æ ·")[
      *Private Key*ï¼š
      - $xi = [xi_1, ..., xi_n]$ N ä¸ª $U[0,1]$ éšæœºå˜é‡
      - $pi$ï¼šè¯è¡¨çš„ä¼ªéšæœºæ’åˆ—

      ç”Ÿæˆç¬¬ $t$ ä¸ª tokenï¼š
      1. LLM â†’ $P("next token")$
      2. ç”¨ $pi$ æ’åˆ— â†’ $P_pi$
      3. è®¡ç®— CDFï¼š$F(k) = sum_(i=1)^k P_pi (i)$
      4. æ‰¾æœ€å° $k$ï¼š$F(k) gt.eq xi_t$
      5. è¿”å› $pi^(-1)(k)$
    ]
  ],
)

#theorem(title: "ä¸ºä»€ä¹ˆæ˜¯ Distortion-Freeï¼Ÿ")[
  probä¸º $p$ çš„ tokenï¼Œè¢«é€‰ä¸­çš„probæ°å¥½ä¹Ÿæ˜¯ $p$ï¼š
  $ P("sample token with prob" p) = P(xi_t "falls in interval of length" p) = p $

  *ä»£ä»·*ï¼šç¡®å®šæ€§è¾“å‡ºï¼ˆåŒ prompt åŒ responseï¼‰ï¼Œå¤šæ ·æ€§ä¸§å¤±ã€‚
]

== SynthID (Google DeepMind)

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === ç‰¹ç‚¹

    - âœ… Distortion-Freeï¼ˆä¿æŒåˆ†å¸ƒï¼‰
    - âœ… Non-Deterministicï¼ˆåŒ prompt å¯å¾—ä¸åŒå›å¤ï¼‰
    - âœ… å¤§è§„æ¨¡éªŒè¯ï¼ˆ2000 ä¸‡æ–‡æœ¬ AB æµ‹è¯•ï¼‰

    === Tournament Sampling

    1. hash(context) + key â†’ $G$ å€¼ï¼ˆ$m$ ä¸ª Bernoulliï¼‰
    2. ä» LLM åˆ†å¸ƒé‡‡æ · $2^m$ ä¸ªå€™é€‰
    3. é”¦æ ‡èµ›ï¼šæ¯è½®æ¯”è¾ƒ $G$ å€¼ï¼Œå¤§è€…æ™‹çº§
    4. Winner â†’ æœ€ç»ˆé‡‡æ ·ï¼ˆé«˜ $G$ å€¼ token æ›´æ˜“èµ¢ï¼‰
  ],
  [
    ```
    Tournamentï¼ˆé”¦æ ‡èµ›ï¼‰ï¼š

    ç¬¬1è½®        ç¬¬2è½®        ç¬¬3è½®
    â”Œâ”€â”€â”€â”       â”Œâ”€â”€â”€â”
    â”‚A,Bâ”‚â”€Gå¤§è€…â”€â”‚   â”‚
    â””â”€â”€â”€â”˜       â”‚W1 â”‚â”€Gå¤§è€…â”€â”Œâ”€â”€â”€â”
    â”Œâ”€â”€â”€â”       â”‚W2 â”‚       â”‚WINâ”‚
    â”‚C,Dâ”‚â”€Gå¤§è€…â”€â””â”€â”€â”€â”˜       â””â”€â”€â”€â”˜
    â””â”€â”€â”€â”˜                 â†’ æœ€ç»ˆé‡‡æ ·

    ç›´è§‰ï¼šé«˜Gå€¼tokenæ›´æ˜“èµ¢å¾—æ¯”èµ›
    ```

    æ£€æµ‹ï¼š$S = sum_t sum_(i=1)^m G_(t,i) tilde "Binomial"(T dot m, 0.5)$
  ],
)

== æ°´å°æ”»å‡»æ–¹æ³•

#grid(
  columns: (1fr, 1fr, 1fr),
  gutter: 0.8em,
  [
    === Scrubbingï¼ˆç§»é™¤ï¼‰

    - çŸ­æ–‡æœ¬ ($lt$ 100 tokens)ï¼šä¿¡å·æœ¬å°±å¼±
    - ä¸­æ–‡æœ¬ (100-600)ï¼šParaphrase ~30% tokens å³å¯
    - é•¿æ–‡æœ¬ ($gt$ 600)ï¼šéœ€ Watermark Stealing
  ],
  [
    === Spoofingï¼ˆä¼ªé€ ï¼‰

    *Piggyback Spoofing*ï¼š

    åŸæ–‡ï¼š"This article is great"
    â†“ æ”¹ä¸€ä¸ªè¯
    æ”»å‡»ï¼š"This article is mean"
    â†’ ä»æœ‰æ°´å°ï¼
  ],
  [
    === Stealingï¼ˆçªƒå–ï¼‰

    1. Query æ°´å° LLM ~30K æ¬¡ï¼ˆçº¦ \$50ï¼‰
    2. ä¼°è®¡ $frac(P_("wm"), P_("base"))$
    3. $S > 0$ â†’ é¢„æµ‹ Green
    4. ç”¨äº Scrubbing/Spoofing
  ],
)

#definition(title: "Watermark Stealing æ ¸å¿ƒå…¬å¼")[
  $ S("token"|"ctx") = log frac(P_("watermarked")("token"|"ctx") + epsilon, P_("base")("token"|"ctx") + epsilon) $

  *Spoofing Detection*åˆ©ç”¨ä¼ªéšæœºæ— æ³•æ³›åŒ–åˆ°ç½•è§ n-gram çš„ç‰¹æ€§ï¼š

  è®¡ç®— Correlation(token é¢œè‰², N-gram é¢‘ç‡)
  - çœŸæ°´å°ï¼šæ— ç›¸å…³æ€§ï¼ˆä¼ªéšæœºä¸é¢‘ç‡æ— å…³ï¼‰
  - ä¼ªé€ ï¼šç½•è§è¯æ›´å®¹æ˜“çŒœé”™ â†’ æœ‰ç›¸å…³æ€§
]

== Radioactivityï¼ˆæ•°æ®ä¿æŠ¤ï¼‰

#theorem(title: "æ ¸å¿ƒå‘ç°")[
  *åœ¨æ°´å°æ•°æ®ä¸Šè®­ç»ƒçš„modelï¼Œè¾“å‡ºä¹Ÿä¼šå¸¦æœ‰æ°´å°ï¼*

  åº”ç”¨ï¼šDataset Inference Attack
  1. ç”¨æ°´å° LLM paraphrase è‡ªå·±çš„æ–‡ç« 
  2. å‘å¸ƒæ°´å°ç‰ˆæœ¬åˆ°ç½‘ä¸Š
  3. æŸ¥è¯¢å¯ç–‘modelï¼Œæ£€æµ‹è¾“å‡ºæ˜¯å¦æœ‰æ°´å°
  4. è‹¥æœ‰ â†’ è¯æ˜è¯¥modelè®­ç»ƒä½¿ç”¨äº†ä½ çš„æ•°æ®
]

== LLM Benchmarking

#grid(
  columns: (1fr, 1.2fr),
  gutter: 1em,
  [
    #definition(title: "Benchmark ä¸‰è¦ç´ ")[
      $ "Benchmark" = ("Task", "Scoring", "Standardized Setup") $

      *ä¸ä¼ ç»Ÿ ML çš„åŒºåˆ«*ï¼š
      - è¯„ä¼°å¯¹è±¡ï¼šAlgorithm â†’ Modelï¼ˆäº§å“ï¼‰
      - Train/Testï¼šIID split â†’ è¾¹ç•Œæ¨¡ç³Š
      - Taskï¼šæ˜ç¡®ï¼ˆåˆ†ç±»ï¼‰â†’ å¼€æ”¾ï¼ˆä»»ä½•é—®é¢˜ï¼‰
      - Accessï¼šå®Œå…¨æ§åˆ¶ â†’ å¸¸å¸¸åªæœ‰ API
    ]
  ],
  [
    === å››å¤§è¯„ä¼°èŒƒå¼

    #figure(
      table(
        columns: 3,
        align: left,
        [*èŒƒå¼*], [*ç­”æ¡ˆæ ¼å¼*], [*è¯„ä¼°æ–¹å¼*],
        [Closed-form], [A/B/C/D], [ç²¾ç¡®åŒ¹é…],
        [Free-form], [è‡ªç”±ç”Ÿæˆ], [éªŒè¯ç»“æœï¼ˆå•å…ƒæµ‹è¯•ï¼‰],
        [Simulation], [ä¸ç¯å¢ƒäº¤äº’], [ç¯å¢ƒåé¦ˆ],
        [Preference], [ä¸¤modelå¯¹æ¯”], [äºº/LLM é€‰æ‹©åå¥½],
      ),
    )
  ],
)

== Contaminationï¼ˆæ±¡æŸ“é—®é¢˜ï¼‰

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    #definition(title: "Data vs Task Contamination")[
      *Data Contamination*#footnote[Benchmark çš„å…·ä½“é—®é¢˜/ç­”æ¡ˆå‡ºç°åœ¨è®­ç»ƒæ•°æ®ä¸­]ï¼š
      - Benchmark é—®é¢˜åœ¨training setä¸­
      - model"èƒŒç­”æ¡ˆ"
      - *æ€§èƒ½è™šé«˜*

      *Task Contamination*ï¼š
      - è®­ç»ƒæ•°æ®é’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¼˜åŒ–
      - å¯èƒ½æ˜¯è‰¯æ€§ï¼ˆé¼“åŠ±modelå˜å¼ºï¼‰
      - ä¹Ÿå¯èƒ½åªå­¦ä¼šæ ¼å¼/å¥—è·¯
    ]
  ],
  [
    === å½¢å¼åŒ–å®šä¹‰

    $ "Contaminated" arrow.l.r.double exists x in D_("train"): F(x, b) > tau $

    å…¶ä¸­ï¼š
    - $b$ï¼šbenchmark sample
    - $x$ï¼šè®­ç»ƒæ•°æ®sample
    - $F$ï¼šç›¸ä¼¼åº¦å‡½æ•°ï¼ˆ*æ ¸å¿ƒéš¾ç‚¹*ï¼‰
    - $tau$ï¼šé˜ˆå€¼

    #tip[
      å®šä¹‰ $F$ éå¸¸éš¾ï¼å®Œå…¨ç›¸åŒï¼Ÿè¯­ä¹‰ç›¸åŒï¼Ÿæ¢ä¸ªè¯´æ³•ç®—ä¸ç®—ï¼Ÿ
    ]
  ],
)

=== æ£€æµ‹æ–¹æ³•ï¼ˆæŒ‰ Access Level åˆ†ç±»ï¼‰

#grid(
  columns: (1fr, 1fr, 1fr),
  gutter: 0.8em,
  [
    === Level 1: Oracle Access

    *æ–¹æ³•*ï¼šN-gram Overlap

    ä» benchmark å’Œè®­ç»ƒæ•°æ®æå– n-gramsï¼Œè®¡ç®— overlap

    *ç¼ºç‚¹*ï¼šç®€å•æ”¹å†™å°±èƒ½ç»•è¿‡
  ],
  [
    === Level 2: White-box

    *æ–¹æ³•*ï¼šPerplexity / Min-K% Prob

    æ ¸å¿ƒç›´è§‰ï¼šmodelå¯¹è§è¿‡çš„sample"å¼‚å¸¸ç¡®ä¿¡"

    *è”ç³»*ï¼šä¸ MIA éå¸¸ç›¸ä¼¼ï¼
  ],
  [
    === Level 3: Black-box

    *æ–¹æ³•*ï¼šCompletion Test

    ç»™model benchmark å‰åŠéƒ¨åˆ†ï¼Œè®©å®ƒè¡¥å…¨

    è‹¥è¡¥å…¨å®Œå…¨ä¸€è‡´ â†’ å¯èƒ½è§è¿‡
  ],
)

=== Outcome-based Detection

#theorem(title: "MathArena ç­–ç•¥")[
  åˆ©ç”¨*æ—¶é—´å› æœæ€§*ï¼š
  $ "Performance Gap" = "Score"_(2024) - "Score"_(2025) gt.double 0 arrow.r.double "Contamination" $

  å‡è®¾modelåœ¨ 2024 å¹´å‰è®­ç»ƒï¼š
  - è‹¥ 2024 é¢˜è¡¨ç°*æ˜¾è‘—ä¼˜äº*2025 é¢˜ï¼ˆåº”åŒåˆ†å¸ƒï¼‰
  - åˆ™è¯æ˜ 2024 é¢˜è¢«è®°å¿†ï¼ˆæ±¡æŸ“ï¼‰

  *è¿™æ˜¯åäº‹å®æ¨æ–­ï¼*
]

== Dynamic Benchmarksï¼ˆåŠ¨æ€è¯„ä¼°ï¼‰

#definition(title: "ä¸ºä»€ä¹ˆéœ€è¦ Dynamic Benchmark?")[
  *Static Benchmark çš„é—®é¢˜*ï¼š
  - å‘å¸ƒåç«‹å³è¢«çˆ¬å–è¿›è®­ç»ƒæ•°æ®
  - modelåœ¨"è€ƒè¯•"å’Œ"è§£å†³é—®é¢˜"ä¸Šåˆ†ä¸æ¸…
  - åˆ†æ•°è™šé«˜ï¼ˆGoodhart's Lawï¼‰
]

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === Dynamic Benchmark ç±»å‹

    #figure(
      table(
        columns: 2,
        align: left,
        [*ç±»å‹*], [*ä¾‹å­*],
        [æ—¶é—´åŠ¨æ€], [MathArenaï¼ˆæ–°æ•°å­¦é¢˜ï¼‰],
        [ç”ŸæˆåŠ¨æ€], [Dynabenchï¼ˆæŒç»­æ›´æ–°ï¼‰],
        [ç§æœ‰åŠ¨æ€], [SEAL Leaderboard],
        [éªŒè¯åŠ¨æ€], [Agent ç¯å¢ƒæµ‹è¯•],
      ),
    )
  ],
  [
    === æ ¸å¿ƒä¼˜åŠ¿

    - *Anti-contamination*ï¼šæ–°é¢˜æ— æ³•æå‰å‡†å¤‡
    - *True generalization*ï¼šæµ‹è¯•*èƒ½åŠ›*è€Œé*è®°å¿†*
    - *Continuous evaluation*ï¼šæŒç»­è·Ÿè¸ªè¿›å±•

    #tip[
      *é—®é¢˜*ï¼šå¯èƒ½ä¸æ—§ benchmark ä¸å¯æ¯”

      *è§£å†³*ï¼šä½¿ç”¨ IRT/Polyrating ç»Ÿä¸€ scale
    ]
  ],
)

=== Polyratingï¼šDe-biasing æ–¹æ³•

#theorem(title: "Polyrating æ ¸å¿ƒæ€æƒ³")[
  *é—®é¢˜*ï¼šJudgeï¼ˆäººç±»/LLMï¼‰æœ‰ç³»ç»Ÿæ€§åè§

  *è§£å†³*ï¼šæ˜¾å¼å»ºæ¨¡ bias å‚æ•°å¹¶ä¼°è®¡+ç§»é™¤

  $
    P("Model" i "wins") = sigma(s_i - s_j + underbrace(b_("length") dot Delta "len" + b_("format") dot Delta "fmt" + dots, "Bias Terms"))
  $

  ä¼°è®¡ $b$ åï¼ŒæŠ¥å‘Š de-biased score $s_i$ã€‚
]

== Scoring Mechanisms

#grid(
  columns: (1fr, 1fr, 1fr),
  gutter: 1em,
  [
    === Goodhart's Law

    #tip[
      "When a measure becomes a target, it ceases to be a good measure."

      ä¾‹å¦‚ï¼šROUGE-N è¯„ç¿»è¯‘
      - è®¡ç®— n-gram overlap
      - é—®é¢˜ï¼šç¿»è¯‘å¯ä»¥æœ‰å¤šç§æ­£ç¡®è¡¨è¾¾
      - modelå­¦ä¼šè¿åˆè¯„åˆ†è€ŒéçœŸæ­£ç¿»è¯‘
    ]
  ],
  [
    === Bradley-Terry Model

    ç»™å®šåå¥½æ•°æ® (A vs B, Winner)ï¼Œæ±‚å…¨å±€æ’åï¼š

    $ P(i "beats" j) = frac(e^(s_i), e^(s_i) + e^(s_j)) = sigma(s_i - s_j) $

    *ä¸ ELO å…³ç³»*ï¼š
    - Bradley-Terryï¼šç²¾ç¡®è§£ï¼ˆå‡¸ä¼˜åŒ–ï¼‰
    - ELOï¼šåœ¨çº¿è¿‘ä¼¼ï¼ˆå¢é‡æ›´æ–°ï¼‰
  ],
  [
    === Judge Bias Problem

    #tip[
      Human/LLM Judge å­˜åœ¨ç³»ç»Ÿæ€§åè§ï¼š
      - âŒ åå¥½æ›´é•¿çš„å›ç­”
      - âŒ åå¥½æ ¼å¼æ›´å¥½çš„å›ç­”ï¼ˆmarkdown, bullet pointsï¼‰
      - âŒ åå¥½æœ‰ emoji çš„å›ç­” ğŸ˜Š
      - âŒ åå¥½æ›´è‡ªä¿¡çš„è¯­æ°”

      *åæœ*ï¼šmodelå­¦ä¼š"è®¨å¥½"è¯„å§”ï¼Œè€ŒéçœŸæ­£å˜å¼º

      *è§£å†³*ï¼šPolyrating æ˜¾å¼å»ºæ¨¡ bias å‚æ•°å¹¶ de-bias
    ]
  ],
)



== Reporting Best Practices

#grid(
  columns: (1fr, 1fr),
  gutter: 1em,
  [
    === åº”è¯¥åšçš„ âœ“

    - æŠ¥å‘Š*ç»Ÿè®¡æ˜¾è‘—æ€§*ï¼ˆ92.15% vs 92.1% å¯èƒ½åªæ˜¯å™ªå£°ï¼‰
    - å…¬å¼€è¯„ä¼°è¾“å‡ºï¼ˆè®©ç¤¾åŒºå¯éªŒè¯ï¼‰
    - Apples-to-apples æ¯”è¾ƒï¼ˆç›¸åŒè®¾ç½®ã€ç›¸åŒ effortï¼‰
    - å¯å¤ç°ï¼ˆè¯¦ç»†è®°å½•é…ç½®ã€éšæœºç§å­ï¼‰
  ],
  [
    === å¸¸è§ä¸è¯šå®æ‰‹æ³• âœ—

    - *Benchmark Omission*ï¼šåªæŠ¥å‘Šå¥½çš„ benchmark
    - *Creative Reporting*ï¼šæŸ±çŠ¶å›¾ä¸ä» 0 å¼€å§‹
    - *Artificial Increase*ï¼šè‡ªå·±modelç²¾å¿ƒè°ƒå‚ï¼Œç«å“é»˜è®¤è®¾ç½®
  ],
)

#grid(
  columns: (1fr, 1fr),
  [
    == Watermarking è¯„ä¼°æŒ‡æ ‡

    #theorem(title: "TPR @ low FPR æ‰æ˜¯æ ¸å¿ƒæŒ‡æ ‡ï¼")[
      ```
               TPR
                â†‘
           1.0 â”€â”¤      â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                â”‚     â•±
                â”‚    â•±  â† åªå…³å¿ƒè¿™é‡Œï¼
                â”‚   â•±
           0.5 â”€â”¤  â•±
                â”‚ â•±
                â”‚â•±
           0.0 â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ FPR
                0  0.001  0.01   1.0
                   â†‘
              FPRæä½æ—¶çš„TPR
      ```

      å…¶ä»–dimï¼šDetectability, Quality, Robustness, Security
    ]
  ],
  [
    == æ˜“é”™ç‚¹

    *Detection éœ€è¦ LLM?*ï¼šâŒ åªéœ€ secret keyï¼Œæ— éœ€ LLMï¼

    *AUC æ˜¯å¥½æŒ‡æ ‡?*ï¼šâŒ åªå…³å¿ƒæä½ FPR ä¸‹çš„ TPR

    *Distortion-Free = æ— å½±å“?*ï¼šæ˜¯*æœŸæœ›æ„ä¹‰*ä¸Šä¸æ”¹å˜åˆ†å¸ƒ

    *æ°´å°è¶Šå¼ºè¶Šå¥½?*ï¼šéœ€æƒè¡¡ Quality å’Œ Detectability

    *é«˜åˆ† = å¥½model?*ï¼šå¯èƒ½æ˜¯æ±¡æŸ“/cherry-picking

    *N-gram æ£€æµ‹å¤Ÿç”¨?*ï¼šç®€å•æ”¹å†™å³å¯ç»•è¿‡

    *å®˜æ–¹æ•°å­—å¯ä¿¡?*ï¼šæ£€æŸ¥è®¾ç½®æ˜¯å¦å…¬å¹³ã€æ˜¯å¦æœ‰é—æ¼
  ],
)
